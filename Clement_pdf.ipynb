{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11366,
     "status": "ok",
     "timestamp": 1745423017846,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "eDFlhQ3VkZwa",
    "outputId": "a1cf844b-aa69-4a0d-d723-2485024fa56d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 11446,
     "status": "ok",
     "timestamp": 1745423029316,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "WBQHB56kh6-9",
    "outputId": "df801438-6de9-4d20-b92d-fcab29ab1063"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2be4899b-8f39-418c-88ac-1be1a7e75221\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2be4899b-8f39-418c-88ac-1be1a7e75221\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Part 1-G1 (Report).pdf to Part 1-G1 (Report).pdf\n",
      "Uploaded file: Part 1-G1 (Report).pdf\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()  # Opens a file picker in Colab\n",
    "file_path = list(uploaded.keys())[0]  # Get the filename\n",
    "\n",
    "print(\"Uploaded file:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4832,
     "status": "ok",
     "timestamp": 1745423035168,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "MLoqsId7iu3T",
    "outputId": "5e03b2e9-c333-4384-8f02-206d1b9dbfe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2518,
     "status": "ok",
     "timestamp": 1745423037690,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "EnHEVgUwi5vd",
    "outputId": "531355a0-4449-4f4d-cd84-60050f74cb60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "   \n",
      "   FACULTY  OF  COMPUTING  AND  INFORMATION  TECHNOLOGY   BMDS2114  Machine  Learning   Assignment   Semester  202501     Programme  (Year  &  Semester)  :  RDS  Y2S3  \n",
      "Tutorial  Group  :  4    Team  members:   No  Name   Registration  No.  Signature  \n",
      "1  Clement  Lau  Kai  Ming  24WMR08858  \n",
      "lau  \n",
      "2  Chow  Zhen  Kit  24WMR08857  chow  \n",
      "3  Edison  Chan  Yoong  Qi  24WMR08859  Chan  \n",
      "4  Gan  She  Ming  24WMR08864  Ming          Photo  and                            Photo  and                           Photo  and                             Photo  and  Name  of  1   Name  of  2   Name  of  3   Name  of  4     \n",
      "Table  of  Contents  1.  Frame  the  Problem  and  Look  at  the  Big  Picture  5 1.1  Background  5 1.2  Problem  Statement  6 1.3  Objectives  8 1.5  Expected  Outcomes  8 1.6  Key  Challenges  9 1.7  Evaluation  Metrics  9 1.8  Complexity  of  the  Environment  10 1.9  Theoretical  Background  10 2.  Define  the  Environment  and  the  Agent  11 2.1  Environment  in  which  the  agent  will  operate  11 2.2  State  Space  11 2.3  Action  State  13 2.4  Reward  Structure  14 2.4.1  Positive  Feedback  14 2.4.2  Negative  Feedback  14 2.5  Agent  Establishment  15 2.6  Reinforcement  Learning  Agent  Components  15 2.6.1  Policy  15 2.6.2  Value  Function  15 2.6.3  Environment  Interaction  16 Interaction  Steps:  16 2.7  Agent  Learning  Objectives  17 2.8  Theoretical  Background  18 2.8.1  Markov  Decision  Process  (MDP)  18 2.8.2  Transition  Space  19 2.8.3  Word  state  transition  19 3.  Get  or  Create  the  Environment  and  define  its  Dynamics  23 3.1  Environment  Selection  and  Design  23 3.1.1  Justification  23 4.  Select  the  Reinforcement  Learning  Algorithm  and  Model  24 4.1  Dyna  Q  24 4.2  Deep  Deterministic  Policy  Gradient  (DDPG)  26 4.3  SARSA  (State-Action-Reward-State-Action)  28 4.4  Deep  Q  Learning  32 4.5  Model  Free  vs  Model  Based  33 4.5.1  Model  Selection  34 5.  Explore  the  Model  and  Short-list  the  Best  Approach  35 5.1  Algorithm  Implementation  35 5.2  Exploration  Strategies  35 5.3  Performance  Across  Different  Hyperparameter  Choices  36 5.4  Strengths,  Weaknesses  and  Improvements  37 5.5  Theoretical  Background  38      \n",
      "   \n",
      "     \n",
      "      Using  Reinforcement  Learning  to  \n",
      "Optimise\n",
      " \n",
      "Vocabulary\n",
      " \n",
      "Selection\n",
      " \n",
      "in\n",
      " \n",
      "English\n",
      " \n",
      "Language\n",
      " \n",
      "Teaching\n",
      " \n",
      " \n",
      "  \n",
      " 1.  Frame  the  Problem  and  Look  at  the  Big  Picture  \n",
      "1.1  Background  \n",
      "In  the  context  of  English  language  teaching,  vocabulary  learning  is  one  of  the  most  important  \n",
      "aspects\n",
      " \n",
      "of\n",
      " \n",
      "mastering\n",
      " \n",
      "a\n",
      " \n",
      "language.\n",
      " \n",
      "With\n",
      " \n",
      "a\n",
      " \n",
      "sufficient\n",
      " \n",
      "vocabulary,\n",
      " \n",
      "learners\n",
      " \n",
      "can\n",
      " \n",
      "aid\n",
      " \n",
      "and\n",
      " \n",
      "deepen\n",
      " \n",
      "comprehension,\n",
      " \n",
      "as\n",
      " \n",
      "well\n",
      " \n",
      "as\n",
      " \n",
      "the\n",
      " \n",
      "ability\n",
      " \n",
      "to\n",
      " \n",
      "express\n",
      " \n",
      "themselves.\n",
      " \n",
      "Despite\n",
      " \n",
      "the\n",
      " \n",
      "importance\n",
      " \n",
      "attached\n",
      " \n",
      "to\n",
      " \n",
      "vocabulary\n",
      " \n",
      "selection,\n",
      " \n",
      "its\n",
      " \n",
      "most\n",
      " \n",
      "appropriate\n",
      " \n",
      "identification\n",
      " \n",
      "at\n",
      " \n",
      "various\n",
      " \n",
      "proficiency\n",
      " \n",
      "levels\n",
      " \n",
      "remains\n",
      " \n",
      "difficult.\n",
      " \n",
      "The\n",
      " \n",
      "selection\n",
      " \n",
      "of\n",
      " \n",
      "vocabulary\n",
      " \n",
      "at\n",
      " \n",
      "lower\n",
      " \n",
      "levels\n",
      " \n",
      "of\n",
      " \n",
      "proficiency\n",
      " \n",
      "is\n",
      " \n",
      "often\n",
      " \n",
      "done\n",
      " \n",
      "with\n",
      " \n",
      "static\n",
      " \n",
      "word\n",
      " \n",
      "lists\n",
      " \n",
      "or\n",
      " \n",
      "frequency\n",
      " \n",
      "of\n",
      " \n",
      "occurrence\n",
      " \n",
      "checklists,\n",
      " \n",
      "which\n",
      " \n",
      "seldom\n",
      " \n",
      "address\n",
      " \n",
      "the\n",
      " \n",
      "specific\n",
      " \n",
      "and\n",
      " \n",
      "ever-changing\n",
      " \n",
      "needs\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "learner.\n",
      " \n",
      "Also,\n",
      " \n",
      "a\n",
      " \n",
      "lot\n",
      " \n",
      "of\n",
      " \n",
      "students\n",
      " \n",
      "find\n",
      " \n",
      "themselves\n",
      " \n",
      "struggling\n",
      " \n",
      "to\n",
      " \n",
      "learn\n",
      " \n",
      "English\n",
      " \n",
      "vocabulary\n",
      " \n",
      "due\n",
      " \n",
      "to\n",
      " \n",
      "a\n",
      " \n",
      "lot\n",
      " \n",
      "of\n",
      " \n",
      "factors\n",
      " \n",
      "such\n",
      " \n",
      "as\n",
      " \n",
      "being\n",
      " \n",
      "less\n",
      " \n",
      "interested\n",
      " \n",
      "or\n",
      " \n",
      "less\n",
      " \n",
      "motivated\n",
      " \n",
      "to\n",
      " \n",
      "learn.\n",
      " \n",
      "English\n",
      " \n",
      "teachers\n",
      " \n",
      "also\n",
      " \n",
      "face\n",
      " \n",
      "challenges\n",
      " \n",
      "to\n",
      " \n",
      "teach\n",
      " \n",
      "English\n",
      " \n",
      "vocabulary\n",
      " \n",
      "to\n",
      " \n",
      "students\n",
      " \n",
      "(Suardi\n",
      " \n",
      "&\n",
      " \n",
      "Sakti,\n",
      " \n",
      "2019,\n",
      " \n",
      "p.\n",
      " \n",
      "100).\n",
      "  Reinforcement  learning  (RL),  a  subfield  of  machine  learning,  holds  great  promise  in  optimizing  \n",
      "vocabulary\n",
      " \n",
      "selection\n",
      " \n",
      "in\n",
      " \n",
      "English\n",
      " \n",
      "language\n",
      " \n",
      "teaching.\n",
      " \n",
      "RL\n",
      " \n",
      "algorithms\n",
      " \n",
      "seek\n",
      " \n",
      "to\n",
      " \n",
      "learn\n",
      " \n",
      "which\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "possible\n",
      " \n",
      "options\n",
      " \n",
      "are\n",
      " \n",
      "best\n",
      " \n",
      "for\n",
      " \n",
      "maximizing\n",
      " \n",
      "total\n",
      " \n",
      "expected\n",
      " \n",
      "payoffs\n",
      " \n",
      "with\n",
      " \n",
      "greater\n",
      " \n",
      "accuracy\n",
      " \n",
      "through\n",
      " \n",
      "the\n",
      " \n",
      "use\n",
      " \n",
      "of\n",
      " \n",
      "rewards\n",
      " \n",
      "given\n",
      " \n",
      "by\n",
      " \n",
      "the\n",
      " \n",
      "environment\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "what\n",
      " \n",
      "works\n",
      " \n",
      "and\n",
      " \n",
      "what\n",
      " \n",
      "does\n",
      " \n",
      "not.\n",
      " \n",
      "In\n",
      " \n",
      "English\n",
      " \n",
      "language\n",
      " \n",
      "teaching,\n",
      " \n",
      "RL\n",
      " \n",
      "can\n",
      " \n",
      "be\n",
      " \n",
      "applied\n",
      " \n",
      "for\n",
      " \n",
      "adaptive\n",
      " \n",
      "vocabulary\n",
      " \n",
      "selection\n",
      " \n",
      "during\n",
      " \n",
      "the\n",
      " \n",
      "learning\n",
      " \n",
      "process,\n",
      " \n",
      "based\n",
      " \n",
      "upon\n",
      " \n",
      "the\n",
      " \n",
      "interaction\n",
      " \n",
      "level\n",
      " \n",
      "of\n",
      " \n",
      "learners,\n",
      " \n",
      "learners'\n",
      " \n",
      "moving\n",
      " \n",
      "grasp\n",
      " \n",
      "within\n",
      " \n",
      "the\n",
      " \n",
      "classroom\n",
      " \n",
      "lesson,\n",
      " \n",
      "and\n",
      " \n",
      "contextual\n",
      " \n",
      "relevance.\n",
      " \n",
      "This\n",
      " \n",
      "could\n",
      " \n",
      "mean\n",
      " \n",
      "more\n",
      " \n",
      "efficient\n",
      " \n",
      "and\n",
      " \n",
      "effective\n",
      " \n",
      "teaching\n",
      " \n",
      "of\n",
      " \n",
      "vocabulary\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "sense\n",
      " \n",
      "of\n",
      " \n",
      "appropriately\n",
      " \n",
      "adapting\n",
      " \n",
      "the\n",
      " \n",
      "content\n",
      " \n",
      "to\n",
      " \n",
      "suit\n",
      " \n",
      "the\n",
      " \n",
      "learner's\n",
      " \n",
      "unique\n",
      " \n",
      "needs.\n",
      "    The  blending  of  RL  in  English  language  teaching  is  especially  in  demand  because  of  the  \n",
      "increasing\n",
      " \n",
      "use\n",
      " \n",
      "of\n",
      " \n",
      "technology\n",
      " \n",
      "in\n",
      " \n",
      "education.\n",
      " \n",
      "In\n",
      " \n",
      "this\n",
      " \n",
      "sense,\n",
      " \n",
      "digital\n",
      " \n",
      "learning\n",
      " \n",
      "platforms\n",
      " \n",
      "and\n",
      " \n",
      "intelligent\n",
      " \n",
      "tutoring\n",
      " \n",
      "systems\n",
      " \n",
      "are\n",
      " \n",
      "an\n",
      " \n",
      "ideal\n",
      " \n",
      "platform\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "incorporation\n",
      " \n",
      "of\n",
      " \n",
      "RL\n",
      " \n",
      "algorithms,\n",
      " \n",
      "in\n",
      " \n",
      "which\n",
      " \n",
      "data\n",
      " \n",
      "could\n",
      " \n",
      "be\n",
      " \n",
      "collected\n",
      " \n",
      "and\n",
      " \n",
      "analysed\n",
      " \n",
      "in\n",
      " \n",
      "real-time.\n",
      " \n",
      "RL\n",
      " \n",
      "enables\n",
      " \n",
      "the\n",
      " \n",
      "development\n",
      " \n",
      "of\n",
      " \n",
      "customised\n",
      " \n",
      "and\n",
      " \n",
      "adaptive\n",
      " \n",
      "learning\n",
      " \n",
      "experiences\n",
      " \n",
      "for\n",
      " \n",
      "teachers\n",
      " \n",
      "for\n",
      " \n",
      "better\n",
      " \n",
      "student\n",
      " \n",
      "outcomes.\n",
      "  Though  promising,  we  are  in  the  early  stages  of  applying  RL  in  English  language  teaching.  The  \n",
      "current\n",
      " \n",
      "bulk\n",
      " \n",
      "of\n",
      " \n",
      "research\n",
      " \n",
      "turns\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "theoretical\n",
      " \n",
      "aspects\n",
      " \n",
      "of\n",
      " \n",
      "RL,\n",
      " \n",
      "while\n",
      " \n",
      "only\n",
      " \n",
      "a\n",
      " \n",
      "handful\n",
      " \n",
      "of\n",
      " experimental  studies  illustrate  actual  applications  of  RL  in  language  education.  Such  an  \n",
      "oversight\n",
      " \n",
      "serves\n",
      " \n",
      "as\n",
      " \n",
      "an\n",
      " \n",
      "opportunity\n",
      " \n",
      "to\n",
      " \n",
      "investigate\n",
      " \n",
      "further\n",
      " \n",
      "how\n",
      " \n",
      "RL\n",
      " \n",
      "can\n",
      " \n",
      "be\n",
      " \n",
      "used\n",
      " \n",
      "to\n",
      " \n",
      "optimise\n",
      " \n",
      "vocabulary\n",
      " \n",
      "selection\n",
      " \n",
      "in\n",
      " \n",
      "English\n",
      " \n",
      "language\n",
      " \n",
      "teaching\n",
      " \n",
      "and\n",
      " \n",
      "knowledge\n",
      " \n",
      "acquisition\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "whole\n",
      " \n",
      "learning\n",
      " \n",
      "experience.\n",
      "  \n",
      "1.2  Problem  Statement   \n",
      "1.  Problem  faced  by  students  while  learning  English  vocabulary:  \n",
      "Students  find  it  very  difficult  to  remember  vocabulary.  To  master  English  vocabulary,  students  \n",
      "have\n",
      " \n",
      "to\n",
      " \n",
      "be\n",
      " \n",
      "able\n",
      " \n",
      "to\n",
      " \n",
      "learn\n",
      " \n",
      "a\n",
      " \n",
      "lot\n",
      " \n",
      "of\n",
      " \n",
      "English\n",
      " \n",
      "words\n",
      " \n",
      "by\n",
      " \n",
      "memorizing.\n",
      " \n",
      "Students\n",
      " \n",
      "tend\n",
      " \n",
      "to\n",
      " \n",
      "forget\n",
      " \n",
      "recently\n",
      " \n",
      "learnt\n",
      " \n",
      "vocabulary\n",
      " \n",
      "when\n",
      " \n",
      "they\n",
      " \n",
      "try\n",
      " \n",
      "to\n",
      " \n",
      "remember\n",
      " \n",
      "new\n",
      " \n",
      "words.\n",
      " \n",
      "One\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "factors\n",
      " \n",
      "is\n",
      " \n",
      "that\n",
      " \n",
      "they\n",
      " \n",
      "did\n",
      " \n",
      "not\n",
      " \n",
      "learn\n",
      " \n",
      "English\n",
      " \n",
      "vocabulary\n",
      " \n",
      "repetitively.\n",
      " \n",
      "Students  find  learning  English  vocabulary  is  boring.  They  were  less  keen  to  learn  it  due  to  no  \n",
      "motivation\n",
      " \n",
      "in\n",
      " \n",
      "learning\n",
      " \n",
      "English.\n",
      " \n",
      "When\n",
      " \n",
      "learning\n",
      " \n",
      "English,\n",
      " \n",
      "students\n",
      " \n",
      "tend\n",
      " \n",
      "to\n",
      " \n",
      "pay\n",
      " \n",
      "less\n",
      " \n",
      "attention\n",
      " \n",
      "to\n",
      " \n",
      "the\n",
      " \n",
      "material\n",
      " \n",
      "taught\n",
      " \n",
      "because\n",
      " \n",
      "they\n",
      " \n",
      "feel\n",
      " \n",
      "lazy\n",
      " \n",
      "and\n",
      " \n",
      "bored\n",
      " \n",
      "during\n",
      " \n",
      "the\n",
      " \n",
      "lessons.\n",
      " \n",
      "This\n",
      " \n",
      "is\n",
      " \n",
      "because\n",
      " \n",
      "they\n",
      " \n",
      "do\n",
      " \n",
      "not\n",
      " \n",
      "feel\n",
      " \n",
      "actively\n",
      " \n",
      "involved\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "class.\n",
      " \n",
      " \n",
      "2.  Problem  faced  by  teachers  while  teaching  English  vocabulary:  \n",
      "The  limitations  of  teacher  judgement.  Even  though  a  student  can  be  tested  by  taking  the  \n",
      "vocabulary\n",
      " \n",
      "test,\n",
      " \n",
      "the\n",
      " \n",
      "teacher\n",
      " \n",
      "is\n",
      " \n",
      "not\n",
      " \n",
      "able\n",
      " \n",
      "to\n",
      " \n",
      "surely\n",
      " \n",
      "identify\n",
      " \n",
      "that\n",
      " \n",
      "the\n",
      " \n",
      "student\n",
      " \n",
      "is\n",
      " \n",
      "able\n",
      " \n",
      "to\n",
      " \n",
      "memorize,\n",
      " \n",
      "understand,\n",
      " \n",
      "and\n",
      " \n",
      "apply\n",
      " \n",
      "the\n",
      " \n",
      "vocabulary\n",
      " \n",
      "that\n",
      " \n",
      "has\n",
      " \n",
      "been\n",
      " \n",
      "taught\n",
      " \n",
      "by\n",
      " \n",
      "the\n",
      " \n",
      "teacher.\n",
      " \n",
      "The  teacher  has  a  limitation  of  the  vocabulary  due  to  the  professional  knowledge  level  of  the  \n",
      "teacher\n",
      " \n",
      "studies.\n",
      " \n",
      "The  teacher  has  a  bias  on  vocabulary  selection.  The  teacher  is  going  to  use  some  inappropriate  \n",
      "vocabulary\n",
      " \n",
      "due\n",
      " \n",
      "to\n",
      " \n",
      "the\n",
      " \n",
      "community\n",
      " \n",
      "culture\n",
      " \n",
      "or\n",
      " \n",
      "personal\n",
      " \n",
      "preference\n",
      " \n",
      "on\n",
      " \n",
      "some\n",
      " \n",
      "specific\n",
      " \n",
      "vocabulary.\n",
      " \n",
      " 3.  Inefficient  Vocabulary  Selection  Methods:  \n",
      "Current  vocabulary  selection  methods  primarily  depend  on  either  static  word  lists  or  \n",
      "frequency-based\n",
      " \n",
      "approaches.\n",
      " \n",
      "These\n",
      " \n",
      "methods\n",
      " \n",
      "do\n",
      " \n",
      "not\n",
      " \n",
      "fully\n",
      " \n",
      "respond\n",
      " \n",
      "to\n",
      " \n",
      "individual\n",
      " \n",
      "learning\n",
      " \n",
      "needs,\n",
      " \n",
      "progress,\n",
      " \n",
      "and\n",
      " \n",
      "contextual\n",
      " \n",
      "relevance\n",
      " \n",
      "for\n",
      " \n",
      "each\n",
      " \n",
      "student,\n",
      " \n",
      "where\n",
      " \n",
      "vocabulary\n",
      " \n",
      "acquisition\n",
      " \n",
      "and\n",
      " \n",
      "engagement\n",
      " \n",
      "become\n",
      " \n",
      "ineffective\n",
      " \n",
      "(Oxford\n",
      " \n",
      "&\n",
      " \n",
      "Crookall,\n",
      " \n",
      "1990,\n",
      " \n",
      "pp.\n",
      " \n",
      "10-12).\n",
      "  This  ineffectiveness  in  vocabulary  selection  poses  major  challenges  for  learners  and  educators  \n",
      "alike.\n",
      " \n",
      "A\n",
      " \n",
      "learner\n",
      " \n",
      "can\n",
      " \n",
      "be\n",
      " \n",
      "actually\n",
      " \n",
      "faced\n",
      " \n",
      "with\n",
      " \n",
      "a\n",
      " \n",
      "set\n",
      " \n",
      "of\n",
      " \n",
      "vocabulary\n",
      " \n",
      "that\n",
      " \n",
      "is\n",
      " \n",
      "either\n",
      " \n",
      "below\n",
      " \n",
      "or\n",
      " \n",
      "above\n",
      " \n",
      "their\n",
      " \n",
      "respective\n",
      " \n",
      "level\n",
      " \n",
      "of\n",
      " \n",
      "ability,\n",
      " \n",
      "with\n",
      " \n",
      "the\n",
      " \n",
      "resultant\n",
      " \n",
      "risk\n",
      " \n",
      "of\n",
      " \n",
      "either\n",
      " \n",
      "boring\n",
      " \n",
      "them\n",
      " \n",
      "to\n",
      " \n",
      "distraction\n",
      " \n",
      "or\n",
      " \n",
      "delaying\n",
      " \n",
      "their\n",
      " \n",
      "progress,\n",
      " \n",
      "resulting\n",
      " \n",
      "in\n",
      " \n",
      "decreased\n",
      " \n",
      "motivation\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "coursework.\n",
      " \n",
      "Educators\n",
      " \n",
      "face\n",
      " \n",
      "difficulty\n",
      " \n",
      "in\n",
      " \n",
      "delivering\n",
      " \n",
      "personalized\n",
      " \n",
      "learning\n",
      " \n",
      "opportunities\n",
      " \n",
      "that\n",
      " \n",
      "address\n",
      " \n",
      "their\n",
      " \n",
      "students'\n",
      " \n",
      "varied\n",
      " \n",
      "needs.\n",
      "  Traditional  vocabulary  selection  uses  a  standardized  approach  and  lacks  the  flexibility  and  \n",
      "adaptability\n",
      " \n",
      "to\n",
      " \n",
      "meet\n",
      " \n",
      "individual\n",
      " \n",
      "learners'\n",
      " \n",
      "dynamic\n",
      " \n",
      "and\n",
      " \n",
      "context-specific\n",
      " \n",
      "needs\n",
      " \n",
      "(McQuillan,\n",
      " \n",
      "2019,\n",
      " \n",
      "p.\n",
      " \n",
      "314).\n",
      "  So,  we  need  to  solve  this  problem  for  improving  vocabulary  teaching.  Teachers  can  quicken  the  \n",
      "learning\n",
      " \n",
      "process\n",
      " \n",
      "and\n",
      " \n",
      "enhance\n",
      " \n",
      "their\n",
      " \n",
      "English\n",
      " \n",
      "through\n",
      " \n",
      "vocabulary\n",
      " \n",
      "selection\n",
      " \n",
      "that\n",
      " \n",
      "mirrored\n",
      " \n",
      "the\n",
      " \n",
      "real-time\n",
      " \n",
      "personalized\n",
      " \n",
      "features\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "learners.\n",
      "           \n",
      " 1.3  Objectives\n",
      " \n",
      "●  To  compare  the  accuracy  of  RL  based  vocabulary  selection  and  traditional  vocabulary  \n",
      "selection\n",
      " \n",
      "in\n",
      " \n",
      "predicting\n",
      " \n",
      "word\n",
      " \n",
      "retention\n",
      " ●  To  evaluate  the  effectiveness  of  RL  in  improving  vocabulary  selection  (recall  rate,  bias  \n",
      "reduction)\n",
      " ●  To  evaluate  the  effectiveness  of  the  RL  system  in  improving  vocabulary  retention  rates  \n",
      "compared\n",
      " \n",
      "to\n",
      " \n",
      "static\n",
      " \n",
      "word\n",
      " \n",
      "lists,\n",
      " \n",
      "using\n",
      " \n",
      "quantitative\n",
      " \n",
      "metrics\n",
      " \n",
      "like\n",
      " \n",
      "accuracy\n",
      " \n",
      "and\n",
      " \n",
      "recall,\n",
      " \n",
      "focusing\n",
      " \n",
      "on\n",
      " \n",
      "reinforcing\n",
      " \n",
      "difficult\n",
      " \n",
      "words\n",
      " \n",
      "by\n",
      " \n",
      "repeating\n",
      " \n",
      "them\n",
      " \n",
      "more\n",
      " \n",
      "frequently\n",
      "  ●  To  design  and  implement  an  adaptive  feedback  mechanism  using  RL  that  dynamically  filters  \n",
      "redundant\n",
      " \n",
      "vocabulary\n",
      " \n",
      "repetitions\n",
      " \n",
      "by\n",
      " \n",
      "tracking\n",
      " \n",
      "learner\n",
      " \n",
      "mastery\n",
      " \n",
      "levels\n",
      " \n",
      "(e.g.\n",
      " \n",
      "via\n",
      " \n",
      "accuracy\n",
      " \n",
      "thresholds)\n",
      " \n",
      "and\n",
      " \n",
      "adjusts\n",
      " \n",
      "content\n",
      " \n",
      "delivery\n",
      " \n",
      "(skip\n",
      " \n",
      "mastered\n",
      " \n",
      "words,\n",
      " \n",
      "more\n",
      " \n",
      "challenging\n",
      " \n",
      "words)\n",
      "  ●  To  develop  a  proficiency-based  vocabulary  selection  system  that  personalise  quizzes  using  \n",
      "RL\n",
      " \n",
      "algorithms\n",
      " \n",
      "calibrated\n",
      " \n",
      "to\n",
      " \n",
      "individual\n",
      " \n",
      "learners’\n",
      " \n",
      "performance\n",
      " \n",
      "data\n",
      " \n",
      "(error\n",
      " \n",
      "rates,\n",
      " \n",
      "response\n",
      " \n",
      "times)\n",
      " \n",
      "to\n",
      " \n",
      "align\n",
      " \n",
      "with\n",
      " \n",
      "their\n",
      " \n",
      "current\n",
      " \n",
      "proficiency\n",
      " \n",
      "level\n",
      "  ●  To  evaluate  the  impact  of  gamified  reward  systems  (points,  badges)  integrated  into  the  RL  \n",
      "framework\n",
      " \n",
      "on\n",
      " \n",
      "learner\n",
      " \n",
      "motivation,\n",
      " \n",
      "using\n",
      " \n",
      "engagement\n",
      " \n",
      "metrics\n",
      " \n",
      "(session\n",
      " \n",
      "duration,\n",
      " \n",
      "completion\n",
      " \n",
      "rates)\n",
      " \n",
      "and\n",
      " \n",
      "post-intervention\n",
      " \n",
      "surveys\n",
      "     \n",
      "1.5  Expected  Outcomes  \n",
      "●  Improved  Vocabulary  Retention.  Students  will  remember  and  use  words  more  effectively  \n",
      "due\n",
      " \n",
      "to\n",
      " \n",
      "personalized\n",
      " \n",
      "reinforcement\n",
      " \n",
      "and\n",
      " \n",
      "spaced\n",
      " \n",
      "repetition.\n",
      " ●  Increase  student  motivation  and  engagement.  Rewards  elements  encourage  students  to  stay  \n",
      "focused\n",
      " \n",
      "and\n",
      " \n",
      "actively\n",
      " \n",
      "involved.\n",
      " ●  Increase  efficiency.  Agent  will  review  students'  vocabulary  instantly  in  a  short  amount  of  \n",
      "time.\n",
      " ●  Reduce  teacher  workload.  With  the  RL  agent  less  teachers  will  be  involved  hence  reduced  \n",
      "staff\n",
      " \n",
      "cost\n",
      " \n",
      "and\n",
      " \n",
      "resources\n",
      " \n",
      "such\n",
      " \n",
      "as\n",
      " \n",
      "class\n",
      " \n",
      "and\n",
      " \n",
      "other\n",
      " \n",
      "facilities.\n",
      " 1.6  Key  Challenges  \n",
      "1.   Exploration  vs  exploitation:  \n",
      "●  The  RL  agent  must  explore  new  vocabularies  to  ensure  diverse  learning  while  also  using  \n",
      "words\n",
      " \n",
      "to\n",
      " \n",
      "benefit\n",
      " \n",
      "the\n",
      " \n",
      "learner/\n",
      " \n",
      "user\n",
      " \n",
      "2.  Sparse  rewards:  \n",
      "●  Measuring  the  effectiveness  of  vocabulary  selection  is  challenging,  as  rewards  (e.g.,  \n",
      "improved\n",
      " \n",
      "retention\n",
      " \n",
      "or\n",
      " \n",
      "comprehension)\n",
      " \n",
      "may\n",
      " \n",
      "not\n",
      " \n",
      "be\n",
      " \n",
      "immediately\n",
      " \n",
      "observable.\n",
      " \n",
      "3.   Long-term  decision-making:  \n",
      "●  Vocabulary  learning  is  a  long  cumulative  process  which  requires  long  term  planning.  \n",
      "Systems\n",
      " \n",
      "like\n",
      " \n",
      "short\n",
      " \n",
      "term\n",
      " \n",
      "reward\n",
      " \n",
      "systems\n",
      " \n",
      "may\n",
      " \n",
      "not\n",
      " \n",
      "correlate\n",
      " \n",
      "with\n",
      " \n",
      "long\n",
      " \n",
      "term\n",
      " \n",
      "retention\n",
      " \n",
      "and\n",
      " \n",
      "usage.\n",
      " \n",
      "  \n",
      "1.7  Evaluation  Metrics  \n",
      "1.  Vocabulary  retention  rate:  ●   Percentage  of  words  correctly  recalled  after  a  delay  e.g.,  after  one  day,  one  week.  \n",
      " 2.  Exploration-Exploitation  Ratio:  ●  Measures  whether  the  agent  is  balancing  new  word  introduction  with  reinforcing  \n",
      "known\n",
      " \n",
      "words.\n",
      " \n",
      "(50%\n",
      " \n",
      "new\n",
      " \n",
      "words,\n",
      " \n",
      "50%\n",
      " \n",
      "learnt\n",
      " \n",
      "words)\n",
      " \n",
      " 3.  Subjective  Learning  Improvement:  ●  Learners'  self-reported  vocabulary  improvement.  \n",
      "     \n",
      "1.8  Complexity  of  the  Environment  \n",
      "The  environment  for  this  RL-based  vocabulary  reinforcement  system  is  a  personalized  learning  \n",
      "platform\n",
      " \n",
      "where\n",
      " \n",
      "students\n",
      " \n",
      "interact\n",
      " \n",
      "with\n",
      " \n",
      "vocabulary\n",
      " \n",
      "exercises,\n",
      " \n",
      "and\n",
      " \n",
      "their\n",
      " \n",
      "responses\n",
      " \n",
      "determine\n",
      " \n",
      "the\n",
      " \n",
      "agent’s\n",
      " \n",
      "decisions.\n",
      " \n",
      "It\n",
      " \n",
      "will\n",
      " \n",
      "continuously\n",
      " \n",
      "update\n",
      " \n",
      "vocabulary\n",
      " \n",
      "recommendations\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "students’\n",
      " \n",
      "performance\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "dynamic\n",
      " \n",
      "and\n",
      " \n",
      "adaptive.\n",
      "  The  RL  agent  needs  to  interact  with  students  more  often  so  that  it  can  analyze  their  responses  \n",
      "and\n",
      " \n",
      "update\n",
      " \n",
      "learning\n",
      " \n",
      "paths,\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "a\n",
      " \n",
      "moderate\n",
      " \n",
      "computing\n",
      " \n",
      "agent.\n",
      " \n",
      "  \n",
      "1.9  Theoretical  Background  \n",
      "The  environment  for  this  RL-based  vocabulary  reinforcement  system  is  a  personalized  learning  \n",
      "platform\n",
      " \n",
      "where\n",
      " \n",
      "students\n",
      " \n",
      "interact\n",
      " \n",
      "with\n",
      " \n",
      "vocabulary\n",
      " \n",
      "exercises,\n",
      " \n",
      "and\n",
      " \n",
      "their\n",
      " \n",
      "responses\n",
      " \n",
      "determine\n",
      " \n",
      "the\n",
      " \n",
      "agent’s\n",
      " \n",
      "decisions.\n",
      " \n",
      "It\n",
      " \n",
      "will\n",
      " \n",
      "continuously\n",
      " \n",
      "update\n",
      " \n",
      "vocabulary\n",
      " \n",
      "recommendations\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "students’\n",
      " \n",
      "performance\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "dynamic\n",
      " \n",
      "and\n",
      " \n",
      "adaptive.\n",
      "  The  RL  agent  needs  to  interact  with  students  more  often  so  that  it  can  analyze  their  responses  \n",
      "and\n",
      " \n",
      "update\n",
      " \n",
      "learning\n",
      " \n",
      "paths,\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "a\n",
      " \n",
      "moderate\n",
      " \n",
      "computing\n",
      " \n",
      "agent.\n",
      " \n",
      " 2.  Define  the  Environment  and  the  Agent  \n",
      "2.1  Environment  in  which  the  agent  will  operate  The  environment  needs  to  simulate  a  student's  learning  process.  The  agent  (RL  model)  will  \n",
      "interact\n",
      " \n",
      "with\n",
      " \n",
      "this\n",
      " \n",
      "environment\n",
      " \n",
      "by\n",
      " \n",
      "selecting\n",
      " \n",
      "words\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "student\n",
      " \n",
      "to\n",
      " \n",
      "learn,\n",
      " \n",
      "receiving\n",
      " \n",
      "feedback,\n",
      " \n",
      "and\n",
      " \n",
      "adjusting\n",
      " \n",
      "its\n",
      " \n",
      "strategy\n",
      " \n",
      "to\n",
      " \n",
      "maximize\n",
      " \n",
      "vocabulary\n",
      " \n",
      "retention.\n",
      "   \n",
      "2.2  State  Space  In  computer  science,  a  state  space  is  a  discrete  space  representing  the  set  of  all  possible  \n",
      "configurations\n",
      " \n",
      "of\n",
      " \n",
      "a\n",
      " \n",
      "system.\n",
      " \n",
      "It\n",
      " \n",
      "is\n",
      " \n",
      "a\n",
      " \n",
      "useful\n",
      " \n",
      "abstraction\n",
      " \n",
      "for\n",
      " \n",
      "reasoning\n",
      " \n",
      "about\n",
      " \n",
      "the\n",
      " \n",
      "behavior\n",
      " \n",
      "of\n",
      " \n",
      "a\n",
      " \n",
      "given\n",
      " \n",
      "system\n",
      " \n",
      "and\n",
      " \n",
      "is\n",
      " \n",
      "widely\n",
      " \n",
      "used\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "fields\n",
      " \n",
      "of\n",
      " \n",
      "artificial\n",
      " \n",
      "intelligence\n",
      " \n",
      "and\n",
      " \n",
      "game\n",
      " \n",
      "theory.\n",
      "  Initial  State  ●  This  state  is  the  state  where  the  agent  will  ask  questions  to  students  to  test  their  English  \n",
      "vocabulary\n",
      " \n",
      "proficiency.\n",
      " ●  This  will  be  initiated  if  a  student  uses  the  agent  for  the  first  time.   New  Word  State  ●  The  state  where  a  word  is  introduced  to  the  student  that  uses  this  agent  for  the  first  time.  ●  This  state  will  be  introduced  in  beginner,  intermediate  and  advanced  vocabulary  learning  \n",
      "states.\n",
      " ●  Student  will  be  transition  to  this  state  if  the  student  is  new  to  the  agent   Familiar  Word  State  ●  The  state  where  a  word  that  was  introduced  before,  and  students  still  get  it  wrong  will  be  \n",
      "introduced\n",
      " \n",
      "again.\n",
      " ●  Students  will  enter  this  state  when  they  incorrectly  recall  a  word  during  quizzes.   Memorised  Word  State  ●  The  state  where  word  is  removed  from  familiar  and  new  word  state  and  only  reintroduced  \n",
      "in\n",
      " \n",
      "a\n",
      " \n",
      "longer\n",
      " \n",
      "periodic\n",
      " \n",
      "revision\n",
      " \n",
      "cycle.\n",
      " ●  Long-term  vocabulary  retention   Low  Proficiency  Level  state  ●  The  state  when  a  student's  proficiency  level  is  low.  ●  In  this  state,  the  agent  will  go  through  New  Word  State,  Familiar  Word  State  and  \n",
      "Memorised\n",
      " \n",
      "Word\n",
      " \n",
      "State.\n",
      " \n",
      "These\n",
      " \n",
      "states\n",
      " \n",
      "will\n",
      " \n",
      "introduce\n",
      " \n",
      "beginner\n",
      " \n",
      "level\n",
      " \n",
      "English\n",
      " \n",
      "vocabularies.\n",
      " \n",
      " ●  Students  will  transition  to  this  state  if  the  student  is  new  to  the  agent  and  the  student  \n",
      "proficiency\n",
      " \n",
      "is\n",
      " \n",
      "low\n",
      " \n",
      "after\n",
      " \n",
      "conducting\n",
      " \n",
      "the\n",
      " \n",
      "initial\n",
      " \n",
      "state.\n",
      "  Intermediate  Proficiency  Level  state  ●  The  state  when  a  student’s  proficiency  level  is  medium  and  shows  progress  beyond  \n",
      "beginner\n",
      " \n",
      "level.\n",
      " ●  In  this  state,  the  agent  will  go  through  New  Word  State,  Familiar  Word  State  and  \n",
      "Memorised\n",
      " \n",
      "Word\n",
      " \n",
      "State.\n",
      " \n",
      "These\n",
      " \n",
      "states\n",
      " \n",
      "will\n",
      " \n",
      "introduce\n",
      " \n",
      "intermediate\n",
      " \n",
      "level\n",
      " \n",
      "English\n",
      " \n",
      "vocabularies.\n",
      " \n",
      " ●  Students  will  transition  to  this  state  if  the  student’s  answers  are  fully  correct  at  Low  \n",
      "Proficiency\n",
      " \n",
      "Level\n",
      " \n",
      "state\n",
      " \n",
      "for\n",
      " \n",
      "3\n",
      " \n",
      "consecutive\n",
      " \n",
      "rows.\n",
      " \n",
      "   Advanced  Proficiency  Level  state  ●  The  state  when  a  student’s  proficiency  level  is  high  and  shows  progress  beyond  \n",
      "intermediate\n",
      " \n",
      "level.\n",
      " ●  In  this  state,  the  agent  will  go  through  New  Word  State,  Familiar  Word  State  and  \n",
      "Memorised\n",
      " \n",
      "Word\n",
      " \n",
      "State.\n",
      " \n",
      "These\n",
      " \n",
      "states\n",
      " \n",
      "will\n",
      " \n",
      "introduce\n",
      " \n",
      "high\n",
      " \n",
      "level\n",
      " \n",
      "English\n",
      " \n",
      "vocabularies.\n",
      " \n",
      " ●  Students  will  transition  to  this  state  if  the  student’s  answers  are  fully  correct  at  \n",
      "Intermediate\n",
      " \n",
      "Proficiency\n",
      " \n",
      "Level\n",
      " \n",
      "state\n",
      " \n",
      "for\n",
      " \n",
      "5\n",
      " \n",
      "consecutive\n",
      " \n",
      "rows.\n",
      " \n",
      "  Reward  State  ●  This  state  is  to  encourage  students  to  have  positive  behaviour  and  be  more  involved.  ●  If  a  student  answers  correctly,  they  earn  points  and  print  motivational  messages.   Penalty  State  ●  If  a  student  answers  a  question  incorrectly,  the  system  will  increase  occurrence  of  \n",
      "Familiar\n",
      " \n",
      "Word\n",
      " \n",
      "State\n",
      " \n",
      "and\n",
      " \n",
      "reduce\n",
      " \n",
      "points.\n",
      "           2.3  Action  State  Ask  Vocabulary  Questions  based  on  proficiency  level  ●  Introduce  new  word  ●  Triggered  during  New  Word  State,  Familiar  Word  State  and  Memorised  Word  State  ●  Test  the  student  on  the  word's  meaning,  listening,  and  recognising.  ●  All  level  state:  This  state  will  occur  in  all  proficiency  levels.   Proficiency  level  quiz  state  ●  The  model  will  ask  vocabulary  questions  of  varying  difficulty  to  gauge  the  initial  \n",
      "proficiency\n",
      " \n",
      "level\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "student.\n",
      " ●  Will  also  test  the  student  at  every  n-th  words  to  see  if  the  student  is  ready  for     Provide  Feedback  ●  Correct:  Give  motivational  and  positive  feedback,  +  points.  ●  Incorrect:  Provide  Hints  and  show  the  correct  answer  -  points.   Adjust  Student’s  Proficiency  Level  ●  To  higher  level:  Answer  correctly  multiple  times  in  a  row  ●  Static:  occasional  mistakes  ●  To  lower  level:  struggles  constantly                 \n",
      " 2.4  Reward  Structure   \n",
      "2.4.1  Positive  Feedback  The  agent  receives  positive  feedback  or  reward  when  it  selects  vocabulary  that  is  appropriate  for  \n",
      "the\n",
      " \n",
      "learners’\n",
      " \n",
      "level\n",
      " \n",
      "and\n",
      " \n",
      "their\n",
      " \n",
      "learning\n",
      " \n",
      "progresses.\n",
      " \n",
      " ●  Improved  proficiency :  If  the  progress  of  the  learner  in  their  vocabulary  knowledge  \n",
      "increases\n",
      " \n",
      "(moving\n",
      " \n",
      "to\n",
      " \n",
      "higher\n",
      " \n",
      "level),\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "rewarded\n",
      " ●  Vocabulary  retention :  If  the  learner  correctly  recalls  and  uses  the  word  after  a  specific  \n",
      "time,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "rewarded\n",
      " ●  Learner  engagement :  If  active  engagement  (completing  exercise/quiz)  is  shown  from  \n",
      "the\n",
      " \n",
      "learner\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "selected\n",
      " \n",
      "vocabulary,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "rewarded\n",
      " ●  Usage  accuracy :  If  the  learner  uses  the  selected  vocabulary  in  context  (exercise/quiz)  \n",
      "correctly,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "rewarded\n",
      " ●  Task  completion :  When  the  learner  completes  the  vocabulary  related  task  correctly  and  \n",
      "successfully,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "rewarded\n",
      "   \n",
      "2.4.2  Negative  Feedback  The  agent  receives  negative  feedback  or  penalty  if  it  selects  vocabulary  that  is  not  suitable  for  \n",
      "their\n",
      " \n",
      "current\n",
      " \n",
      "proficiency\n",
      " \n",
      "level\n",
      " \n",
      "or\n",
      " \n",
      "the\n",
      " \n",
      "vocabulary\n",
      " \n",
      "is\n",
      " \n",
      "irrelevant\n",
      " \n",
      "to\n",
      " \n",
      "the\n",
      " \n",
      "context.\n",
      " \n",
      " ●  No  measurable  progress :  If  the  selected  vocabulary  does  not  show  any  measurable  \n",
      "improvement\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "learners’\n",
      " \n",
      "proficiency\n",
      " \n",
      "level\n",
      " \n",
      "(zero\n",
      " \n",
      "improvement\n",
      " \n",
      "on\n",
      " \n",
      "quiz\n",
      " \n",
      "scores\n",
      " \n",
      "or\n",
      " \n",
      "vocabulary\n",
      " \n",
      "retention),\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "penalised\n",
      " ●  Overly  complex  vocabulary :  If  the  agent  suggest  a  word  that  is  way  too  complex  for  \n",
      "learners’\n",
      " \n",
      "level,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "penalised\n",
      " ●  Low  engagement :  If  failed  engagement  (ignore  exercise/quiz,  don’t  attempt  to  use  the  \n",
      "word\n",
      " \n",
      "in\n",
      " \n",
      "context)\n",
      " \n",
      "is\n",
      " \n",
      "shown\n",
      " \n",
      "from\n",
      " \n",
      "the\n",
      " \n",
      "learner\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "selected\n",
      " \n",
      "vocabulary,\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "penalised\n",
      " \n",
      " ●  Wrong  usage :  If  the  learner  makes  mistakes  using  the  vocabulary,  the  agent  is  penalised  ●  Task  failure :  If  the  learner  fails  task  that  are  related  to  the  selected  vocabulary  (fail  the  \n",
      "quiz,\n",
      " \n",
      "not\n",
      " \n",
      "completing\n",
      " \n",
      "the\n",
      " \n",
      "task),\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "penalised\n",
      "         \n",
      "2.5  Agent  Establishment  The  reinforcement  learning  agent  in  this  system  serves  as  the  decision-maker  for  personalized  \n",
      "vocabulary\n",
      " \n",
      "selection\n",
      " \n",
      "and\n",
      " \n",
      "vocabulary\n",
      " \n",
      "reinforcement.\n",
      " \n",
      "The\n",
      " \n",
      "agent’s\n",
      " \n",
      "main\n",
      " \n",
      "duty\n",
      " \n",
      "is\n",
      " \n",
      "to\n",
      " \n",
      "dynamically\n",
      " \n",
      "adjust\n",
      " \n",
      "vocabulary\n",
      " \n",
      "learning\n",
      " \n",
      "pathways\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "learner\n",
      " \n",
      "performance\n",
      " \n",
      "and\n",
      " \n",
      "result,\n",
      " \n",
      "so\n",
      " \n",
      "that\n",
      " \n",
      "it\n",
      " \n",
      "can\n",
      " \n",
      "optimize\n",
      " \n",
      "both\n",
      " \n",
      "vocabulary\n",
      " \n",
      "retention\n",
      " \n",
      "and\n",
      " \n",
      "engagement\n",
      " \n",
      "of\n",
      " \n",
      "learners.\n",
      "   \n",
      "2.6  Reinforcement  Learning  Agent  Components  \n",
      "2.6.1  Policy  The  policy  defines  how  the  RL  agent  chooses  which  word  to  introduce  next.  Basically  the  agent  \n",
      "chooses\n",
      " \n",
      "which\n",
      " \n",
      "action\n",
      " \n",
      "to\n",
      " \n",
      "do\n",
      " \n",
      "next\n",
      " \n",
      "(choosing\n",
      " \n",
      "word).\n",
      " \n",
      "Based\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "student\n",
      " \n",
      "performance/\n",
      " \n",
      "proficiency,\n",
      " \n",
      "the\n",
      " \n",
      "model\n",
      " \n",
      "can\n",
      " \n",
      "have\n",
      " \n",
      "2\n",
      " \n",
      "policies:\n",
      " ●  It  can  have  a  deterministic  policy,  which  selects  the  \"best\"  word  based  on  learned  \n",
      "knowledge.\n",
      " ●  It  can  also  be  a  stochastic  policy,  it  will  select  words  to  balance  exploration  (trying  new  \n",
      "words)\n",
      " \n",
      "and\n",
      " \n",
      "exploitation\n",
      " \n",
      "(choosing\n",
      " \n",
      "the\n",
      " \n",
      "best-known\n",
      " \n",
      "word).\n",
      "   \n",
      "2.6.2  Value  Function  \n",
      "The  value  function  estimates  how  \"good\"  a  given  state,  which  is  a  student's  vocabulary  \n",
      "knowledge,\n",
      " \n",
      "is\n",
      " \n",
      "for\n",
      " \n",
      "achieving\n",
      " \n",
      "long-term\n",
      " \n",
      "learning\n",
      " \n",
      "goals.\n",
      " \n",
      "The\n",
      " \n",
      "model\n",
      " \n",
      "can\n",
      " \n",
      "have\n",
      " \n",
      "1\n",
      " \n",
      "value\n",
      " \n",
      "functions\n",
      " \n",
      "which\n",
      " \n",
      "is:\n",
      " \n",
      "●  Action  Value  Function,  Q(s,a) :  Estimates  the  expected  reward  for  taking  a  particular  \n",
      "action\n",
      " \n",
      "⍺\n",
      " \n",
      "in\n",
      " \n",
      "a\n",
      " \n",
      "given\n",
      " \n",
      "state\n",
      " \n",
      "s\n",
      " \n",
      "and\n",
      " \n",
      "then\n",
      " \n",
      "following\n",
      " \n",
      "a\n",
      " \n",
      "certain\n",
      " \n",
      "policy.\n",
      " \n",
      "This  incentives  the  model  to  do  what's  best  for  the  student  whilst  balancing  exploitation  and  \n",
      "exploration\n",
      " \n",
      "which\n",
      " \n",
      "leads\n",
      " \n",
      "to\n",
      " \n",
      "an\n",
      " \n",
      "optimal\n",
      " \n",
      "knowledge\n",
      " \n",
      "gained\n",
      " \n",
      "for\n",
      " \n",
      "the\n",
      " \n",
      "student.\n",
      " \n",
      "  \n",
      " 2.6.3  Environment  Interaction  \n",
      "The  agent  interacts  with  the  learning  environment,  selecting  words  and  observing  feedback.  \n",
      "Interaction  Steps:  \n",
      "1.  Observe  State  ( )   𝑠\n",
      "𝑡\n",
      "●  The  student’s  current  knowledge  of  words.  \n",
      " 2.  Select  Action  ( )  𝑎\n",
      "𝑡\n",
      "●  Pick  the  next  word  to  introduce.  \n",
      " 3.  Apply  Action  in  Environment  ●  The  student  tries  to  learn  the  word.  \n",
      " 4.  Get  Reward  (  𝑅\n",
      "𝑡\n",
      ")●   Based  on  whether  the  student  remembers  it.  \n",
      " 5.  Update  Value  Function   ●  Adjust  the  policy  to  select  better  words  in  the  future.  \n",
      " 6.  Repeat  Until  Learning  Goal  is  Achieved  \n",
      "     \n",
      " 2.7  Agent  Learning  Objectives  1.  Maximize  the  vocabulary  retention  by  optimizing  word  selection  and  repetition  \n",
      "frequency.\n",
      " 2.  Personalize  the  vocabulary  learning  that  aligns  with  the  individual  proficiency  levels.  3.  Reduce  the  redundancy  to  the  words  that  are  already  mastered,  so  that  it  can  improve  the  \n",
      "learning\n",
      " \n",
      "efficiency.\n",
      " 4.  Increase  the  learner  engagement  by  balancing  difficulty  and  motivation  through  gamified  \n",
      "elements\n",
      " \n",
      "such\n",
      " \n",
      "as\n",
      " \n",
      "the\n",
      " \n",
      "Achievement\n",
      " \n",
      "or\n",
      " \n",
      "Badges.\n",
      " 5.  Continuously  adapt  the  vocabulary  selection  process  based  on  the  real-time  learner  \n",
      "performance\n",
      " \n",
      "and\n",
      " \n",
      "result.\n",
      " \n",
      "      \n",
      " 2.8  Theoretical  Background\n",
      " \n",
      "2.8.1  Markov  Decision  Process  (MDP)  Markov  Decision  Process  is  a  situation  in  which  an  agent  repeats  to  decide  the  best  action  \n",
      "to\n",
      " \n",
      "select\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "its\n",
      " \n",
      "current\n",
      " \n",
      "state.\n",
      " \n",
      "Here\n",
      " \n",
      "is\n",
      " \n",
      "the\n",
      " \n",
      "core\n",
      " \n",
      "concepts\n",
      " \n",
      "of\n",
      " \n",
      "Markov\n",
      " \n",
      "Decision\n",
      " \n",
      "Process:\n",
      " \n",
      "States :   \n",
      "●  Represent  the  different  situations  or  conditions  the  system  can  be  in.   \n",
      "Actions :   \n",
      "●  The  choices  or  operations  an  agent  can  take  in  a  given  state.   \n",
      "Transition  Probabilitie s:   \n",
      "●  The  likelihood  of  transitioning  from  one  state  to  another  after  taking  a  specific  action.   \n",
      "Rewards :   \n",
      "●  A  numerical  value  assigned  to  each  state  transition,  indicating  the  desirability  of  that  \n",
      "outcome.\n",
      " \n",
      " \n",
      "Markov  Property :   \n",
      "●  The  future  state  depends  only  on  the  current  state  and  action,  not  on  the  sequence  of  past  \n",
      "states\n",
      " \n",
      "and\n",
      " \n",
      "actions.\n",
      " \n",
      "         \n",
      " 2.8.2  Transition  Space  \n",
      "This  defines  a  state  transition  to  another  state.  \n",
      " \n",
      "2.8.3  Word  state  transition  \n",
      "This  emphasize  the  transition  from  a  state  to  another  \n",
      "Current  State  Action  Taken  Next  State  Transition  Condition  \n",
      "Initial  State  Assess  student  proficiency  through  a  quiz  \n",
      "Low,  Intermediate,  or  Advanced  Proficiency  State  \n",
      "Based  on  the  initial  quiz  results  \n",
      "Low  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (beginner-level  words)  \n",
      "New  Word  State  +  \n",
      "Memorised  word  state  \n",
      "If  student  encounters  new  words  \n",
      "Low  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (beginner-level  words)  \n",
      "Familiar  Word  State  +  New  Word  State   \n",
      "+   \n",
      "Memorised  word  state  \n",
      "If  student  struggles  with  the  same  words  repeatedly  \n",
      " \n",
      " Current  State  Action  Taken  Next  State  Transition  Condition  \n",
      "Low  Proficiency  Level  \n",
      "Student  answers  correctly  multiple  times  \n",
      "Intermediate  Proficiency  Level  \n",
      "3  consecutive  correct  answers  \n",
      "Intermediate  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (intermediate-level  words)  \n",
      "New  Word  State  +  \n",
      "Memorised  word  state  \n",
      "If  student  encounters  new  words  \n",
      "Intermediate  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (intermediate-level  words)  \n",
      "Familiar  Word  State   +  New  Word  State  +  Memorised  word  state  \n",
      "If  student  struggles  with  certain  words  \n",
      "Intermediate  Proficiency  Level  \n",
      "Student  answers  correctly  multiple  times  \n",
      "Advanced  Proficiency  Level  \n",
      "5  consecutive  correct  answers  \n",
      "Advanced  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (advanced-level  words)  \n",
      "New  Word  State  +  Memorised  Word  state  \n",
      "If  student  encounters  new  words  \n",
      "  \n",
      "Current  State  Action  Taken  Next  State  Transition  Condition  \n",
      "Advanced  Proficiency  Level  \n",
      "Ask  vocabulary  questions  (advanced-level  words)  \n",
      "Familiar  Word  State  +  New  word  state  +  Memorised  word  state  \n",
      "If  student  struggles  with  certain  words  \n",
      "Familiar  Word  State  Repeat  wrong  answered  words  \n",
      "Memorized  Word  State  +  Familiar  Word  State  \n",
      "If  student  answers  correctly  multiple  times  \n",
      "Memorized  Word  State  \n",
      "Periodic  review  of  mastered  words  \n",
      "Low,  Intermediate,  or  Advanced  Proficiency  Level  \n",
      "Based  on  overall  performance  over  time  \n",
      "New  Word  State  Introduce  a  new  word  Familiar  Word  State  If  student  answers  incorrectly  \n",
      "New  Word  State  Introduce  a  new  word  Memorized  Word  State  \n",
      "If  student  answers  correctly  multiple  times  \n",
      " \n",
      "    Current  State  Action  Taken  Next  State  Transition  Condition  \n",
      "Reward  State  Provide  points,  badges,  or  motivational  feedback  \n",
      "No  transition  (remains  in  same  state)  \n",
      "If  student  answers  correctly  \n",
      "Penalty  State  Increase  occurrence  of  Familiar  Word  State  &  reduce  points  \n",
      "Familiar  Word  State  If  student  answers  incorrectly  \n",
      " \n",
      "             \n",
      " 3.  Get  or  Create  the  Environment  and  define  its  Dynamics  \n",
      "3.1  Environment  Selection  and  Design  To  stimulate  “Using  Reinforcement  Learning  to  Optimise  Vocabulary  Selection  in  English  \n",
      "Language\n",
      " \n",
      "Teaching\n",
      " \n",
      "”,\n",
      " \n",
      "we\n",
      " \n",
      "have\n",
      " \n",
      "decided\n",
      " \n",
      "to\n",
      " \n",
      "use\n",
      " \n",
      "OpenAI’s\n",
      " \n",
      "Gym\n",
      " \n",
      "Framework\n",
      " \n",
      "to\n",
      " \n",
      "create\n",
      " \n",
      "a\n",
      " \n",
      "custom\n",
      " \n",
      "environment.\n",
      "  OpenAI  Gym  gives  a  diverse  method  for  building,  implementing  and  evaluating  lots  of  \n",
      "reinforcement\n",
      " \n",
      "learning\n",
      " \n",
      "models.\n",
      " \n",
      "Hence,\n",
      " \n",
      "this\n",
      " \n",
      "is\n",
      " \n",
      "the\n",
      " \n",
      "perfect\n",
      " \n",
      "choice\n",
      " \n",
      "for\n",
      " \n",
      "our\n",
      " \n",
      "adaptive\n",
      " \n",
      "vocabulary\n",
      " \n",
      "learning\n",
      " \n",
      "system.\n",
      "  \n",
      "3.1.1  Justification Standardize  framework  OpenAI  Gym  provides  the  fastest  and  effortless  way  to  define  our  RL  environments  so  that  we  \n",
      "can\n",
      " \n",
      "compare\n",
      " \n",
      "RL\n",
      " \n",
      "algorithms.\n",
      "   Modularity  Open  AI  Gym  provides  us  with  enough  and  sufficient  resources  to  build  an  environment  that  \n",
      "models\n",
      " \n",
      "the\n",
      " \n",
      "dynamics\n",
      " \n",
      "of\n",
      " \n",
      "vocabulary\n",
      " \n",
      "learning\n",
      " \n",
      "including\n",
      " \n",
      "the\n",
      " \n",
      "Markov\n",
      " \n",
      "Decision\n",
      " \n",
      "Process.\n",
      "   Compatibility  Open  AI  Gym  is  compatible  with  many  RL  algorithms.    Quick  Implementation  and  experiment  Open  AI  Gym  allows  quick  testing  and  prototyping  since  it  uses  simulated  training  instead  of  \n",
      "real-time\n",
      " \n",
      "human\n",
      " \n",
      "interaction,\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "an\n",
      " \n",
      "efficient\n",
      " \n",
      "way\n",
      " \n",
      "to\n",
      " \n",
      "implement\n",
      " \n",
      "RL\n",
      " \n",
      "algorithms.\n",
      "       4.  Select  the  Reinforcement  Learning  Algorithm  and  Model\n",
      " \n",
      "4.1  Dyna  Q  \n",
      " Dyna  algorithm  uses  and  integrates  model-free  and  model-based  learning  through:  \n",
      "Key  Components:   \n",
      "1.  Environment  Collection  and  Interaction:  ●  The  Agent  will  interact  with  the  environment  and  collect  experiences  following  \n",
      "the\n",
      " \n",
      "Markov\n",
      " \n",
      "Decision\n",
      " \n",
      "Process\n",
      " \n",
      "(MDP)\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "form\n",
      " \n",
      "of\n",
      " \n",
      "states,\n",
      " \n",
      "action,\n",
      " \n",
      "reword,\n",
      " \n",
      "transition.\n",
      " ●  These  gained  experience  is  used  to  update  the  Q-values  just  like  Q-  Learning  \n",
      " \n",
      "2.  Model  Learning  and  building:  ●  The  agent  will  build  a  model  to  suit  the  environment  from  the  environment  \n",
      "collection\n",
      " \n",
      "for\n",
      " \n",
      "learning\n",
      " \n",
      "purposes\n",
      " \n",
      "including\n",
      " \n",
      "transition\n",
      " \n",
      "dynamics\n",
      " \n",
      "and\n",
      " \n",
      "reward\n",
      " \n",
      "function.\n",
      " \n",
      " \n",
      "3.  Planning  with  Simulated  Experience:  ●  After  building  the  model,  the  agent  will  generate  simulated  experiences  using  the  \n",
      "model\n",
      " \n",
      "to\n",
      " \n",
      "perform\n",
      " \n",
      "simulation\n",
      " \n",
      "and\n",
      " \n",
      "predictions.\n",
      " ●  These  simulation  experiences  and  information  will  be  used  to  perform  extra  \n",
      "tuning\n",
      " \n",
      "to\n",
      " \n",
      "the\n",
      " \n",
      "functions\n",
      " \n",
      "or\n",
      " \n",
      "policies\n",
      " \n",
      "and\n",
      " \n",
      "update\n",
      " \n",
      "the\n",
      " \n",
      "Q-table.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Algorithm  \n",
      "In  the  context  of  reinforcement  learning,  Q-Learning  is  one  of  the  strongest  and  fundamental  \n",
      "reinforcement\n",
      " \n",
      "learning\n",
      " \n",
      "algorithms.\n",
      " \n",
      "However,\n",
      " \n",
      "its\n",
      " \n",
      "slow\n",
      " \n",
      "convergence\n",
      " \n",
      "is\n",
      " \n",
      "a\n",
      " \n",
      "result\n",
      " \n",
      "of\n",
      " \n",
      "over\n",
      " \n",
      "reliance\n",
      " \n",
      "on\n",
      " \n",
      "real-world\n",
      " \n",
      "experiences.\n",
      " \n",
      "Each\n",
      " \n",
      "experience\n",
      " \n",
      "involves\n",
      " \n",
      "conducting\n",
      " \n",
      "the\n",
      " \n",
      "Markov\n",
      " \n",
      "Decision\n",
      " \n",
      "Process\n",
      " \n",
      "(MDP).\n",
      " \n",
      "Dyna-Q\n",
      " \n",
      "addresses\n",
      " \n",
      "this\n",
      " \n",
      "problem\n",
      " \n",
      "very\n",
      " \n",
      "well\n",
      " \n",
      "by\n",
      " \n",
      "integrating\n",
      " \n",
      "with\n",
      " \n",
      "model-based\n",
      " \n",
      "learning\n",
      " \n",
      "into\n",
      " \n",
      "this\n",
      " \n",
      "technique,\n",
      " \n",
      "by\n",
      " \n",
      "incorporating\n",
      " \n",
      "models\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "environment\n",
      " \n",
      "with\n",
      " \n",
      "transition\n",
      " \n",
      "function\n",
      " \n",
      "T\n",
      " \n",
      "and\n",
      " \n",
      "reward\n",
      " \n",
      "function\n",
      " \n",
      "R.\n",
      " \n",
      " \n",
      "1.  Initialize  Selection  ●  Initialize  Q （ s,a)  values  for  all  state-action  pairs.  2.  Loop:  For  each  time  step  ●  Action  Selection:  select  action  based  on  policy  (epsilon-greedy)  ●  Environment  Interaction:  execute  action  a  and  observe  the  reward  r  and  next  \n",
      "state\n",
      " \n",
      "s’\n",
      " ●  Q-Learning  Update:  Q(s,a)=Q(s,a)+ α[ r+ γ  (max/a’)Q(s  ′  ,a  ′  )−Q(s,a)]  ●  Model  Update:  Use  experience  to  update  the  model  of  the  environment  ●  Planning  step:  ○  Randomly  sample  a  state-action  pair  (s,a)  observed  previously  ○  Predict  or  simulate  the  next  state  s’  and  reward  r  using  the  learned  model  ○  Perform  Q-Learning  update  step  previously  using  simulated  experience  \n",
      "Benefits  \n",
      "1.  High  Efficiency  a.  Because  this  algorithm  utilises  the  usage  of  simulated  experiences,  it  is  more  \n",
      "time-space\n",
      " \n",
      "efficient\n",
      " \n",
      "and\n",
      " \n",
      "can\n",
      " \n",
      "learn\n",
      " \n",
      "more\n",
      " \n",
      "quickly.\n",
      " 2.  Flexibility  and  adaptability  a.  This  algorithm  can  adapt  to  changes  as  it  keeps  updating  the  model  3.  Diversity  a.  This  algorithm  uses  both  model-free  and  model-based  approaches.  This  improves  \n",
      "the\n",
      " \n",
      "performance\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "algorithm\n",
      " 4.2  Deep  Deterministic  Policy  Gradient  (DDPG)   DDPG  is  a  model-free,  off-policy  actor-critic  reinforcement  learning  algorithm  for  continuous  \n",
      "action\n",
      " \n",
      "space.\n",
      " \n",
      "It\n",
      " \n",
      "is\n",
      " \n",
      "a\n",
      " \n",
      "successor\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "deterministic\n",
      " \n",
      "policy\n",
      " \n",
      "gradient\n",
      " \n",
      "algorithms\n",
      " \n",
      "that\n",
      " \n",
      "apply\n",
      " \n",
      "deep\n",
      " \n",
      "learning\n",
      " \n",
      "techniques.\n",
      "  Key  Components  \n",
      "1.  Environment  Interaction  ●  Agents  interact  with  the  environments,  collecting  experiences  in  the  form  of  \n",
      "tuples,\n",
      " \n",
      "including\n",
      " \n",
      "state,\n",
      " \n",
      "action,\n",
      " \n",
      "reward,\n",
      " \n",
      "next\n",
      " \n",
      "state,\n",
      " \n",
      "as\n",
      " \n",
      "like\n",
      " \n",
      "the\n",
      " \n",
      "Markov\n",
      " \n",
      "Decision\n",
      " \n",
      "Process\n",
      " \n",
      "(MDP).\n",
      " ●  All  collected  experiences  are  stored  in  a  replay  buffer  and  then  used  for  training.  2.  Actor-Critic  Architecture  ●  Actor  network  learns  a  deterministic  policy  that  maps  states  to  actions.  ●  Critic  network  learns  to  approximate  the  Q-value  function,  which  evaluates  the  \n",
      "quality\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "action\n",
      " \n",
      "taken.\n",
      " 3.  Experience  Replay  ●  Past  experience  saved  in  the  reply  buffer  and  sampled  uniformly  at  random.  4.  Target  Networks  ●  Separating  the  target  network  for  both  actor  and  critic  makes  learning  more  stable  \n",
      "because\n",
      " \n",
      "it\n",
      " \n",
      "is\n",
      " \n",
      "able\n",
      " \n",
      "to\n",
      " \n",
      "reduce\n",
      " \n",
      "the\n",
      " \n",
      "oscillations\n",
      " \n",
      "during\n",
      " \n",
      "training.\n",
      " \n",
      "Algorithm  \n",
      "DDPG  is  the  variant  of  Deep  Q  Learning  but  built  for  continuous  action  spaces.  \n",
      "It  has  an  actor-critic  setup  in  which  the  actor  learns  the  policy  deterministically  and  the  critic  \n",
      "learns\n",
      " \n",
      "the\n",
      " \n",
      "Q-value\n",
      " \n",
      "function.\n",
      " \n",
      "1.  Initialize  ●  Randomly  initialize  the  actor  and  critic  network  weights.  ●  Make  the  target  network  as  copies  of  the  original  networks.  ●  Reinitialize  replay  buffer.   2.  Each  time  step  (Loop)  ●  Select  action  by  the  actor  network  with  added  noise  for  more  exploration.  ●  Perform  the  action,  receive  the  reward,  and  the  transition  to  the  next  state.  ●  Store  transitions  in  the  reply  buffer.  ●  Sample  out  a  minibatch  of  experiences  from  the  replay  buffer.  ●  Compute  target  Q-value  using  the  target  critic  network.  (as  shown  in  Formula1 )   Formula1  =  {  Q_target  =  r  +  γ  *  Q_target(s’,  a’)  }  ●  Minimizing  the  error  between  Q_target  and  Q(s,a)  to  update  the  critic.  ●  Use  policy  gradients  to  update  the  actor  network.  (as  shown  in  Formula2 )  Formula2  =  {  ∇ θ J  ≈  E[ ∇ aQ(s,  a) ∇ θμ( s)]  }  ●  Soft  update  the  target  networks  (as  shown  in  Formula3 )  ●  Formula3  =  {  θ_ target  ←  τθ  +  (1  -  τ)θ_ target  }    Benefits  \n",
      "1.  Able  to  work  for  Continuous  Action  Space,  unlike  Q  Learning-based  methods  that  \n",
      "require\n",
      " \n",
      "discretization,\n",
      " \n",
      "DDPG\n",
      " \n",
      "works\n",
      " \n",
      "in\n",
      " \n",
      "continuous\n",
      " \n",
      "domains.\n",
      "  2.  Able  to  stabilized  learning  with  the  target  networks   3.  Have  a  high  efficient  learning  with  the  experience  replay  that  can  reduce  variance.   4.  Have  a  deterministic  policy  with  noise  for  exploration  purposes  such  as  \n",
      "Ornstein-Uhlenbeck\n",
      " \n",
      "noise.\n",
      "           \n",
      " 4.3  SARSA  (State-Action-Reward-State-Action)  SARSA  is  an  on-policy  temporal  difference  reinforcement  learning  algorithm  that  derives  action  \n",
      "values\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "policy\n",
      " \n",
      "being\n",
      " \n",
      "executed\n",
      " \n",
      "at\n",
      " \n",
      "any\n",
      " \n",
      "point\n",
      " \n",
      "in\n",
      " \n",
      "time.\n",
      " \n",
      "The\n",
      " \n",
      "algorithm\n",
      " \n",
      "can\n",
      " \n",
      "be\n",
      " \n",
      "used\n",
      " \n",
      "for\n",
      " \n",
      "estimating\n",
      " \n",
      "values\n",
      " \n",
      "of\n",
      " \n",
      "state-action\n",
      " \n",
      "pairs\n",
      " \n",
      "and,\n",
      " \n",
      "thus,\n",
      " \n",
      "improving\n",
      " \n",
      "the\n",
      " \n",
      "agent's\n",
      " \n",
      "interaction\n",
      " \n",
      "with\n",
      " \n",
      "the\n",
      " \n",
      "environment\n",
      " \n",
      "and\n",
      " \n",
      "decision-making\n",
      " \n",
      "processes.\n",
      " \n",
      "In\n",
      " \n",
      "the\n",
      " \n",
      "case\n",
      " \n",
      "of\n",
      " \n",
      "English\n",
      " \n",
      "language\n",
      " \n",
      "learning,\n",
      " \n",
      "SARSA\n",
      " \n",
      "becomes\n",
      " \n",
      "a\n",
      " \n",
      "framework\n",
      " \n",
      "for\n",
      " \n",
      "dynamically\n",
      " \n",
      "selecting\n",
      " \n",
      "vocabulary\n",
      " \n",
      "items,\n",
      " \n",
      "personalising\n",
      " \n",
      "learning\n",
      " \n",
      "paths,\n",
      " \n",
      "and\n",
      " \n",
      "devising\n",
      " \n",
      "optimised\n",
      " \n",
      "immediate\n",
      " \n",
      "and\n",
      " \n",
      "long-term\n",
      " \n",
      "retention\n",
      " \n",
      "plans\n",
      " \n",
      "for\n",
      " \n",
      "learners\n",
      " \n",
      "(Velusamy\n",
      " \n",
      "et\n",
      " \n",
      "al.,\n",
      " \n",
      "2013,\n",
      " \n",
      "p.\n",
      " \n",
      "2312).\n",
      " \n",
      "SARSA\n",
      " \n",
      "is\n",
      " \n",
      "a\n",
      " \n",
      "model-free\n",
      " \n",
      "algorithm\n",
      " \n",
      "so\n",
      " \n",
      "it\n",
      " \n",
      "does\n",
      " \n",
      "not\n",
      " \n",
      "require\n",
      " \n",
      "a\n",
      " \n",
      "model\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "environment\n",
      " \n",
      "like\n",
      " \n",
      "exact\n",
      " \n",
      "rules\n",
      " \n",
      "of\n",
      " \n",
      "how\n",
      " \n",
      "a\n",
      " \n",
      "learner's\n",
      " \n",
      "performance\n",
      " \n",
      "evolves\n",
      " \n",
      "over\n",
      " \n",
      "time,\n",
      " \n",
      "instead\n",
      " \n",
      "it\n",
      " \n",
      "learns\n",
      " \n",
      "directly\n",
      " \n",
      "from\n",
      " \n",
      "experience.\n",
      "   Core  Components  of  SARSA  \n",
      "State  (S)   The  learner's  current  knowledge  state  in  the  environment:  ●  Mastery  levels  of  known  words    ●  Recent  exercise  performance  like  accuracy  or  response  time    ●  Session  progress  and  engagement  metrics     Action  (A)  The  action  taken  by  the  agent:  ●  Introducing  new  vocabulary   ●  Scheduling  reviews  of  learned  words    ●  Adjusting  difficulty  levels     Reward  (R)  The  reward  received  after  taking  action  A  in  state  S,  it  is  captured  through:  ●  Immediate  feedback  during  exercise  correctness  or  time  spent    ●  Medium-term  outcomes  like  session  completion  rates    ●  Long-term  goals  like  retention  test  performance     Next  State  (S’)  The  agent  transitions  to  a  new  state  (S’)  based  on  the  action  A  in  state  S   Next  Action  (A’)  The  new  action  taken  by  the  agent  in  the  next  state  based  on  its  policy   Algorithm  \n",
      "Van  Seijen  et  al.  (2009)  state  that  SARSA  uses  the  Q-value  (or  action-value  function)  Q(s,a)  to  \n",
      "estimate\n",
      " \n",
      "the\n",
      " \n",
      "expected\n",
      " \n",
      "future\n",
      " \n",
      "rewards\n",
      " \n",
      "for\n",
      " \n",
      "taking\n",
      " \n",
      "an\n",
      " \n",
      "action\n",
      " \n",
      "A\n",
      " \n",
      "in\n",
      " \n",
      "state\n",
      " \n",
      "S.\n",
      " \n",
      "The\n",
      " \n",
      "agent\n",
      " \n",
      "updates\n",
      " \n",
      "its\n",
      " \n",
      "Q-values\n",
      " \n",
      "after\n",
      " \n",
      "every\n",
      " \n",
      "step\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "the\n",
      " \n",
      "following\n",
      " \n",
      "formula:\n",
      " \n",
      "   𝑄 ( 𝑠\n",
      "𝑡\n",
      ", 𝑎\n",
      "𝑡\n",
      ")  ←  𝑄 ( 𝑠\n",
      "𝑡\n",
      ", 𝑎\n",
      "𝑡\n",
      ")  +  α [ 𝑟 +  γ 𝑄 ( 𝑠\n",
      "𝑡\n",
      "+\n",
      "1\n",
      ", 𝑎\n",
      "𝑡\n",
      "+\n",
      "1\n",
      ")  −  𝑄 ( 𝑠\n",
      "𝑡\n",
      ", 𝑎\n",
      "𝑡\n",
      ") ] Environment  Interaction   ●  The  agent  interacts  directly  with  learners,  collecting  live  experience  tuples  in  the  format   (S,  \n",
      "A,\n",
      " \n",
      "R,\n",
      " \n",
      "S',\n",
      " \n",
      "A').\n",
      " \n",
      "These\n",
      " \n",
      "tuples\n",
      " \n",
      "are\n",
      " \n",
      "used\n",
      " \n",
      "immediately\n",
      " \n",
      "to\n",
      " \n",
      "update\n",
      " \n",
      "Q-values\n",
      " \n",
      "by\n",
      " \n",
      "applying\n",
      " \n",
      "the\n",
      " \n",
      "SARSA\n",
      " \n",
      "temporal\n",
      " \n",
      "difference\n",
      " \n",
      "rule,\n",
      " \n",
      "update\n",
      " \n",
      "the\n",
      " \n",
      "action\n",
      " \n",
      "selection\n",
      " \n",
      "probabilities\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "ε-\n",
      "greedy\n",
      " \n",
      "policy,\n",
      " \n",
      "and\n",
      " \n",
      "keep\n",
      " \n",
      "track\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "learner-specific\n",
      " \n",
      "patterns\n",
      " \n",
      "of\n",
      " \n",
      "vocabulary\n",
      " \n",
      "retention.\n",
      " \n",
      "●  Unlike  model-based  approaches,  SARSA  performs  instant  online  updates  after  each  learner  \n",
      "interaction,\n",
      " \n",
      "progressive\n",
      " \n",
      "policy\n",
      " \n",
      "improvement\n",
      " \n",
      "through\n",
      " \n",
      "direct\n",
      " \n",
      "reward\n",
      " \n",
      "feedback\n",
      " \n",
      "and\n",
      " \n",
      "adaptive\n",
      " \n",
      "exploration\n",
      " \n",
      "via\n",
      " \n",
      "ε-\n",
      "decay\n",
      " \n",
      "(starts\n",
      " \n",
      "exploratory,\n",
      " \n",
      "becomes\n",
      " \n",
      "increasingly\n",
      " \n",
      "greedy)\n",
      "  Model  Learning  \n",
      "1.  Implicit  Policy  Representation  ●  Uses  a  Q-table  to  remember  expected  rewards  for  state-action  pairs  ●  Slowly  constructs  an  optimal  vocabulary  presentation  policy  through:  ○  Positive  reinforcement,  in  which  high-reward  actions  are  assigned  increasing  values  \n",
      "of\n",
      " \n",
      "Q\n",
      " \n",
      " ○  Negative  reinforcement,  in  which  words  that  perform  poorly  are  assigned  decreasing  \n",
      "values\n",
      " \n",
      "of\n",
      " \n",
      "Q\n",
      "  2.  Dynamic  Adaptation  Mechanism   ●  Adjusting  the  conditions  by:  ○  Detecting  patterns  in  error  rates  of  the  individual  ○  Identifying  the  optimum  word  difficulty  sequence  ○  Individualising  review  schedules  according  to  the  history  of  learners'  interactions   Refinement  of  Policies  Based  on  Real  Experiences   1.  Continuous  Policy  Optimisation  ●  The  agent  refines  its  vocabulary  selection  strategy  based  on:  ○  Real  learner  interactions,  which  include  the  state  transitions  ○  Immediate  update  of  Q-value  after  each  action  ●  For  example,  when  a  learner  has  problems  with  a  word  (low  reward),  its  Q-value  decreases,  \n",
      "thereby\n",
      " \n",
      "reducing\n",
      " \n",
      "the\n",
      " \n",
      "probability\n",
      " \n",
      "of\n",
      " \n",
      "being\n",
      " \n",
      "selected\n",
      " \n",
      "in\n",
      " \n",
      "the\n",
      " \n",
      "future\n",
      "  2.  Progressive  Exploration  Tuning  ●  Uses  ε- decay  to  automatically  shift:  ○  Early  phase  (high  ε):  Broad  vocabulary  exploration  ○  Late  phase  (low  ε):  Focus  on  high-value  words   3.  Adaptive  Personalisation  ●  Q-values  change  as  an  individual  learns:  ○  Optimal  word  difficulty  progression   ○  Personalised  schedule  for  reviewing  previously  taught  words   Benefits  \n",
      "1.  Adaptation  during  exploration:   Since  SARSA  uses  the  agent  policy  to  learn  updates,  it  directly  accounts  for  the  \n",
      "exploration-exploitation\n",
      " \n",
      "trade-off.\n",
      " \n",
      "If\n",
      " \n",
      "the\n",
      " \n",
      "agent\n",
      " \n",
      "is\n",
      " \n",
      "exploring\n",
      " \n",
      "more\n",
      " \n",
      "(due\n",
      " \n",
      "to\n",
      " \n",
      "a\n",
      " \n",
      "higher\n",
      " \n",
      "ε\n",
      " \n",
      "value\n",
      " \n",
      "in\n",
      " \n",
      "ε-\n",
      "greedy\n",
      " \n",
      "exploration),\n",
      " \n",
      "that\n",
      " \n",
      "will\n",
      " \n",
      "be\n",
      " \n",
      "also\n",
      " \n",
      "reflected\n",
      " \n",
      "in\n",
      " \n",
      "learning,\n",
      " \n",
      "making\n",
      " \n",
      "SARSA\n",
      " \n",
      "suitable\n",
      " \n",
      "for\n",
      " \n",
      "environments\n",
      " \n",
      "where\n",
      " \n",
      "exploration\n",
      " \n",
      "is\n",
      " \n",
      "important\n",
      "  2.  Stability  in  Stochastic  Environments  \n",
      "Being  an  on-policy  algorithm,  SARSA  takes  into  consideration  environmental  variations  and  \n",
      "hence\n",
      " \n",
      "is\n",
      " \n",
      "suitable\n",
      " \n",
      "for\n",
      " \n",
      "adjustments\n",
      " \n",
      "to\n",
      " \n",
      "the\n",
      " \n",
      "recommendations\n",
      " \n",
      "made\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "student\n",
      " \n",
      "interactions\n",
      " \n",
      "in\n",
      " \n",
      "real\n",
      " \n",
      "time,\n",
      " \n",
      "much\n",
      " \n",
      "like\n",
      " \n",
      "an\n",
      " \n",
      "adaptive\n",
      " \n",
      "system\n",
      " \n",
      "that\n",
      " \n",
      "customizes\n",
      " \n",
      "the\n",
      " \n",
      "learning\n",
      " \n",
      "content\n",
      " \n",
      "based\n",
      " \n",
      "on\n",
      " \n",
      "individual\n",
      " \n",
      "performance\n",
      " \n",
      " 3.  Risk-Averse  Learning  Paths  \n",
      "In  this  approach,  SARSA  updates  Q-values  by  using  the  actual  actions  taken  including  \n",
      "exploratory\n",
      " \n",
      "ones,\n",
      " \n",
      "and\n",
      " \n",
      "its\n",
      " \n",
      "policies\n",
      " \n",
      "are\n",
      " \n",
      "considered\n",
      " \n",
      "to\n",
      " \n",
      "be\n",
      " \n",
      "safe\n",
      " \n",
      "rather\n",
      " \n",
      "than\n",
      " \n",
      "optimal.\n",
      " \n",
      "In\n",
      " \n",
      "the\n",
      " \n",
      "educational\n",
      " \n",
      "context,\n",
      " \n",
      "this\n",
      " \n",
      "means\n",
      " \n",
      "the\n",
      " \n",
      "gradual\n",
      " \n",
      "progression\n",
      " \n",
      "of\n",
      " \n",
      "difficulty\n",
      " \n",
      "avoid\n",
      " \n",
      "overwhelming\n",
      " \n",
      "students\n",
      " \n",
      "with\n",
      " \n",
      "sudden\n",
      " \n",
      "jumps\n",
      " \n",
      "in\n",
      " \n",
      "complexity\n",
      " \n",
      " Tradeoff  1.  Conservative  Progression  Since  SARSA  is  risk  averse,  it  can  cause  delayed  learning  as  policies  that  are  slow  to  expose  the  \n",
      "student\n",
      " \n",
      "to\n",
      " \n",
      "concepts\n",
      " \n",
      "may\n",
      " \n",
      "hinder\n",
      " \n",
      "superior\n",
      " \n",
      "students\n",
      " \n",
      "from\n",
      " \n",
      "progressing\n",
      " \n",
      "on\n",
      " \n",
      "schedule\n",
      " \n",
      " 2.  Sensitive  to  Exploration  Strategy  \n",
      "The  performance  of  the  algorithm  depends  a  lot  on  the  exploration  and  exploitation.  When  ε  (the  \n",
      "exploration\n",
      " \n",
      "rate)\n",
      " \n",
      "is\n",
      " \n",
      "kept\n",
      " \n",
      "very\n",
      " \n",
      "high\n",
      " \n",
      "or\n",
      " \n",
      "very\n",
      " \n",
      "low,\n",
      " \n",
      "either\n",
      " \n",
      "the\n",
      " \n",
      "learned\n",
      " \n",
      "knowledge\n",
      " \n",
      "is\n",
      " \n",
      "not\n",
      " \n",
      "exploited\n",
      " \n",
      "properly,\n",
      " \n",
      "or\n",
      " \n",
      "there\n",
      " \n",
      "is\n",
      " \n",
      "not\n",
      " \n",
      "enough\n",
      " \n",
      "exploration\n",
      " \n",
      "to\n",
      " \n",
      "discover\n",
      " \n",
      "a\n",
      " \n",
      "better\n",
      " \n",
      "policy\n",
      " \n",
      " 3.  High  Noise  or  Uncertainty  Are  Less  Stable  Noisy  environments  are  SARSA's  comfort  zones,  but  it  still  suffers  greatly  where  environments  \n",
      "are\n",
      " \n",
      "highly\n",
      " \n",
      "dynamic\n",
      " \n",
      "or\n",
      " \n",
      "uncertain.\n",
      " \n",
      "The\n",
      " \n",
      "fact\n",
      " \n",
      "that\n",
      " \n",
      "its\n",
      " \n",
      "updates\n",
      " \n",
      "are\n",
      " \n",
      "all\n",
      " \n",
      "oriented\n",
      " \n",
      "towards\n",
      " \n",
      "the\n",
      " \n",
      "current\n",
      " policy  may  lead  to  radically  different  or  even  poor  outcomes  when  environment  dynamics  alter  \n",
      "with\n",
      " \n",
      "great\n",
      " \n",
      "rapidity\n",
      "               \n",
      "4.4  Deep  Q  Learning  Deep  Q-Learning  integrates  deep  neural  networks  into  the  decision-making  process.  This  \n",
      "combination\n",
      " \n",
      "allows\n",
      " \n",
      "agents\n",
      " \n",
      "to\n",
      " \n",
      "handle\n",
      " \n",
      "high-dimensional\n",
      " \n",
      "state\n",
      " \n",
      "spaces,\n",
      " \n",
      "making\n",
      " \n",
      "it\n",
      " \n",
      "possible\n",
      " \n",
      "to\n",
      " \n",
      "solve\n",
      " \n",
      "complex\n",
      " \n",
      "tasks\n",
      " \n",
      "such\n",
      " \n",
      "as\n",
      " \n",
      "playing\n",
      " \n",
      "video\n",
      " \n",
      "games\n",
      " \n",
      "or\n",
      " \n",
      "controlling\n",
      " \n",
      "robots.\n",
      " \n",
      " \n",
      "Environment  Collection  and  Interaction:  \n",
      "●  The  agent  observes  the  state  from  the  environment.  ●  It  selects  an  action  based  on  a  policy  (often  an  epsilon-greedy  policy  for  \n",
      "exploration).\n",
      " ●  The  environment  responds  with  a  reward  and  the  next  state   ●  The  agent  stores  the  experience  tuple  in  a  replay  buffer  to  break  correlations  \n",
      "between\n",
      " \n",
      "consecutive\n",
      " \n",
      "experiences.\n",
      " ●  Uses  an  epsilon-greedy  strategy  to  balance  exploration  (trying  new  actions)  and  \n",
      "exploitation\n",
      " \n",
      "(using\n",
      " \n",
      "known\n",
      " \n",
      "good\n",
      " \n",
      "actions).\n",
      " \n",
      "Model  Learning:  ●  This  phase  involves  learning  an  approximation  of  the  Q-value  function  using  a  neural  \n",
      "network.\n",
      " \n",
      "Planning  with  Simulated  Experience:  \n",
      "●  Instead  of  relying  only  on  real-world  interactions,  DQN  can  improve  sample  efficiency  \n",
      "by\n",
      " \n",
      "using\n",
      " \n",
      "simulated\n",
      " \n",
      "experiences.\n",
      " ●  Replay  Buffer  ○  Stores  past  experiences  and  allows  reusing  them  for  training.  ●  Target  Network:  ○  A  separate  Q-network  with  fixed  parameters  for  stability,  updated  periodically  to  \n",
      "reduce\n",
      " \n",
      "fluctuations\n",
      " \n",
      "in\n",
      " \n",
      "learning.\n",
      " ●  Prioritized  Experience  Replay  (PER):  ○  Instead  of  random  sampling,  experiences  are  replayed  based  on  their  importance,  \n",
      "prioritizing\n",
      " \n",
      "experiences\n",
      " \n",
      "with\n",
      " \n",
      "higher\n",
      " \n",
      "temporal\n",
      " \n",
      "difference\n",
      " \n",
      "(TD)\n",
      " \n",
      "error\n",
      " \n",
      "for\n",
      " \n",
      "better\n",
      " \n",
      "learning.\n",
      " \n",
      "4.5  Model  Free  vs  Model  Based  General  Overview:  Model  Free  Model  Based  \n",
      "The  agent  learns  through  trial  and  error  by  interacting  with  the  environment.  \n",
      "The  agent  first  learns  a  model  of  the  environment’s  dynamics:  P(s′ ∣ s,a)  (how  actions  lead  to  future  states).  \n",
      "It  does  not  try  to  predict  future  states  explicitly.  Instead,  it  directly  learns  an  optimal  policy  or  Q-values.  \n",
      "It  uses  this  model  to  simulate  future  states  and  rewards  to  plan  better  actions.  \n",
      "Examples:  Q-Learning,  DQN,  PPO,  A2C,  SAC  \n",
      "Examples:  AlphaGo  (Monte  Carlo  Tree  Search),  Dyna-Q,  MuZero    Pros:  Model  Free  Model  Based  Easier  to  implement,  works  well  in  complex  environments  \n",
      "More  sample  efficient  (can  learn  from  simulated  experiences)  \n",
      "No  need  to  explicitly  model  environment  dynamics  \n",
      "Faster  convergence  once  the  model  is  accurate  \n",
      "Can  handle  stochastic  (random)  environments  Can  generalize  better  in  some  cases    Cons:  Model  Free  Model  Based  \n",
      "Requires  a  lot  of  interactions  with  the  environment  (sample  inefficient)  \n",
      "Harder  to  implement  (requires  learning  an  accurate  model)  \n",
      "Can  take  longer  to  converge  to  an  optimal  policy  \n",
      "If  the  learned  model  is  inaccurate,  performance  suffers       \n",
      "4.5.1  Model  Selection  Model  Free  is  more  suitable  for  our  reinforcement  learning  project  because:  ●  Vocabulary  learning  has  stochastic  elements  as  students  don’t  always  learn  in  a  \n",
      "predictable\n",
      " \n",
      "way.\n",
      " ●  It’s  difficult  to  model  how  a  student  learns  exactly.  Which  requires  the  model  to  have  \n",
      "many\n",
      " \n",
      "samples.\n",
      " ●  Other  considerations,  having  a  model  based  approach  requires  the  model  to  have  a  very  \n",
      "detailed\n",
      " \n",
      "understanding\n",
      " \n",
      "of\n",
      " \n",
      "the\n",
      " \n",
      "learners\n",
      " \n",
      "assessment\n",
      " \n",
      "which\n",
      " \n",
      "requires\n",
      " \n",
      "much\n",
      " \n",
      "computational\n",
      " \n",
      "power.\n",
      "         5.  Explore  the  Model  and  Short-list  the  Best  Approach  \n",
      "5.1  Algorithm  Implementation   [  no  code  available  to  continue  ]  [  to  do  later  ]              \n",
      "5.2  Exploration  Strategies   e.g.,  ε- greedy,  random  exploration,  entropy-based  exploration,  UCB,  or  Thompson  Sampling            \n",
      " 5.3  Performance  Across  Different  Hyperparameter  Choices   (e.g.,  learning  rate,  discount  factor).                   \n",
      " 5.4  Strengths,  Weaknesses  and  Improvements    [  Identify  strengths,  weaknesses,  and  areas  for  improvement  in  the  chosen  approach.  ]                 \n",
      " 5.5  Theoretical  Background   [  Theoretical  Background:  Discuss  the  exploration  vs.  exploitation  tradeoff  and  strategies  like  \n",
      "ε-\n",
      "greedy,\n",
      " \n",
      "UCB,\n",
      " \n",
      "and\n",
      " \n",
      "Thompson\n",
      " \n",
      "Sampling.\n",
      " \n",
      "]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(file_path)\n",
    "\n",
    "# printing number of pages in pdf file\n",
    "print(len(reader.pages))\n",
    "\n",
    "# getting a specific page from the pdf file\n",
    "all_text  =\"\";\n",
    "for page in reader.pages:\n",
    "  all_text += page.extract_text();\n",
    "\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1745423045807,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "XP5TCrejkkD6",
    "outputId": "3b017b37-471e-43e2-c050-e705933a40c7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6633,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1523,\n        \"samples\": [\n          \"like\",\n          \"pair\",\n          \"Easier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-95f34422-5187-4411-b11b-85b4908c7d19\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FACULTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMPUTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFORMATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TECHNOLOGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BMDS2114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Assignment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95f34422-5187-4411-b11b-85b4908c7d19')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-95f34422-5187-4411-b11b-85b4908c7d19 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-95f34422-5187-4411-b11b-85b4908c7d19');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-79573462-8574-4dbd-b83a-0a53aadfd612\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79573462-8574-4dbd-b83a-0a53aadfd612')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-79573462-8574-4dbd-b83a-0a53aadfd612 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          word\n",
       "0      FACULTY\n",
       "1           OF\n",
       "2    COMPUTING\n",
       "3          AND\n",
       "4  INFORMATION\n",
       "5   TECHNOLOGY\n",
       "6     BMDS2114\n",
       "7      Machine\n",
       "8     Learning\n",
       "9   Assignment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(all_text)\n",
    "\n",
    "df = pd.DataFrame(words, columns=['word'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745423056300,
     "user": {
      "displayName": "LAU KAI MING CLEMENT",
      "userId": "14788290562283705456"
     },
     "user_tz": -480
    },
    "id": "fAB2AN3VlgTf",
    "outputId": "14b7de2a-9ccc-4724-9462-591445b248ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word\n",
      "0         FACULTY\n",
      "1              OF\n",
      "2       COMPUTING\n",
      "3             AND\n",
      "4     INFORMATION\n",
      "...           ...\n",
      "6628            ,\n",
      "6629          and\n",
      "6630     Thompson\n",
      "6631    Sampling.\n",
      "6632            ]\n",
      "\n",
      "[6633 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgUEeHSb9JOvFH/bgmJ3NJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
