{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "# Downlaod nltk packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "import uuid \n",
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A significant subset of natural language data includes documents that span thousands of tokens.\n",
      "The ability to process such long sequences is critical for many NLP tasks including document classification, summarization, multi-hop, and opendomain question answering, and document-level or\n",
      "multi-document relationship extraction and coreference resolution. These tasks have important\n",
      "practical applications in domains such as scientific\n",
      "document understanding and the digital humanities (Ammar et al., 2018; Cohan et al., 2018; Kocisky et al. ´ , 2018; Lo et al., 2020; Wang et al.,\n",
      "2020a). Yet, scaling state-of-the-art models to\n",
      "long sequences is challenging as many models are\n",
      "designed and tested for shorter sequences. One\n",
      "notable example is transformer models (Vaswani\n",
      "et al., 2017) that have O(N2\n",
      ") computational cost in\n",
      "the sequence length N, making them prohibitively\n",
      "expensive to run for many long sequence tasks.\n",
      "This is reflected in many widely-used models such\n",
      "as RoBERTa and BERT where the sequence length\n",
      "is limited to only 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "with open(\"paragraphs.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A significant subset of natural language data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The ability to process such long sequences is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These tasks have important\\npractical applicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>´ , 2018; Lo et al., 2020; Wang et al.,\\n2020a).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet, scaling state-of-the-art models to\\nlong ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences\n",
       "0  A significant subset of natural language data ...\n",
       "1  The ability to process such long sequences is ...\n",
       "2  These tasks have important\\npractical applicat...\n",
       "3   ´ , 2018; Lo et al., 2020; Wang et al.,\\n2020a).\n",
       "4  Yet, scaling state-of-the-art models to\\nlong ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "df = pd.DataFrame(sentences, columns=[\"Sentences\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleanning Function\n",
    "def clean_text(text):\n",
    "    # Convert into lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra blanks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a significant subset of natural language data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the ability to process such long sequences is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these tasks have important practical applicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lo et al wang et al a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yet scaling state of the art models to long se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences\n",
       "0  a significant subset of natural language data ...\n",
       "1  the ability to process such long sequences is ...\n",
       "2  these tasks have important practical applicati...\n",
       "3                              lo et al wang et al a\n",
       "4  yet scaling state of the art models to long se..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentences\"] = df[\"Sentences\"].map(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization with Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentences_Tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a significant subset of natural language data ...</td>\n",
       "      <td>[significant, subset, natural, language, data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the ability to process such long sequences is ...</td>\n",
       "      <td>[ability, process, long, sequences, critical, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these tasks have important practical applicati...</td>\n",
       "      <td>[tasks, important, practical, applications, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lo et al wang et al a</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yet scaling state of the art models to long se...</td>\n",
       "      <td>[yet, scaling, state, art, models, long, seque...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  \\\n",
       "0  a significant subset of natural language data ...   \n",
       "1  the ability to process such long sequences is ...   \n",
       "2  these tasks have important practical applicati...   \n",
       "3                              lo et al wang et al a   \n",
       "4  yet scaling state of the art models to long se...   \n",
       "\n",
       "                                  Sentences_Tokenize  \n",
       "0  [significant, subset, natural, language, data,...  \n",
       "1  [ability, process, long, sequences, critical, ...  \n",
       "2  [tasks, important, practical, applications, do...  \n",
       "3                         [lo, et, al, wang, et, al]  \n",
       "4  [yet, scaling, state, art, models, long, seque...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Treebank tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)  \n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  \n",
    "    return filtered_tokens\n",
    "\n",
    "df[\"Sentences_Tokenize\"] = df[\"Sentences\"].apply(tokenize_tokens)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentences_Tokenize</th>\n",
       "      <th>Stemming_Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a significant subset of natural language data ...</td>\n",
       "      <td>[significant, subset, natural, language, data,...</td>\n",
       "      <td>[signific, subset, natur, languag, data, inclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the ability to process such long sequences is ...</td>\n",
       "      <td>[ability, process, long, sequences, critical, ...</td>\n",
       "      <td>[abil, process, long, sequenc, critic, mani, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these tasks have important practical applicati...</td>\n",
       "      <td>[tasks, important, practical, applications, do...</td>\n",
       "      <td>[task, import, practic, applic, domain, scient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lo et al wang et al a</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yet scaling state of the art models to long se...</td>\n",
       "      <td>[yet, scaling, state, art, models, long, seque...</td>\n",
       "      <td>[yet, scale, state, art, model, long, sequenc,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  \\\n",
       "0  a significant subset of natural language data ...   \n",
       "1  the ability to process such long sequences is ...   \n",
       "2  these tasks have important practical applicati...   \n",
       "3                              lo et al wang et al a   \n",
       "4  yet scaling state of the art models to long se...   \n",
       "\n",
       "                                  Sentences_Tokenize  \\\n",
       "0  [significant, subset, natural, language, data,...   \n",
       "1  [ability, process, long, sequences, critical, ...   \n",
       "2  [tasks, important, practical, applications, do...   \n",
       "3                         [lo, et, al, wang, et, al]   \n",
       "4  [yet, scaling, state, art, models, long, seque...   \n",
       "\n",
       "                                  Stemming_Sentences  \n",
       "0  [signific, subset, natur, languag, data, inclu...  \n",
       "1  [abil, process, long, sequenc, critic, mani, n...  \n",
       "2  [task, import, practic, applic, domain, scient...  \n",
       "3                         [lo, et, al, wang, et, al]  \n",
       "4  [yet, scale, state, art, model, long, sequenc,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "def stemming_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "df[\"Stemming_Sentences\"] = df[\"Sentences_Tokenize\"].apply(stemming_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Sentences_Tokenize</th>\n",
       "      <th>Stemming_Sentences</th>\n",
       "      <th>Lemmatized_Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a significant subset of natural language data ...</td>\n",
       "      <td>[significant, subset, natural, language, data,...</td>\n",
       "      <td>[signific, subset, natur, languag, data, inclu...</td>\n",
       "      <td>[significant, subset, natural, language, data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the ability to process such long sequences is ...</td>\n",
       "      <td>[ability, process, long, sequences, critical, ...</td>\n",
       "      <td>[abil, process, long, sequenc, critic, mani, n...</td>\n",
       "      <td>[ability, process, long, sequence, critical, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these tasks have important practical applicati...</td>\n",
       "      <td>[tasks, important, practical, applications, do...</td>\n",
       "      <td>[task, import, practic, applic, domain, scient...</td>\n",
       "      <td>[task, important, practical, applications, dom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lo et al wang et al a</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "      <td>[lo, et, al, wang, et, al]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yet scaling state of the art models to long se...</td>\n",
       "      <td>[yet, scaling, state, art, models, long, seque...</td>\n",
       "      <td>[yet, scale, state, art, model, long, sequenc,...</td>\n",
       "      <td>[yet, scale, state, art, model, long, sequence...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  \\\n",
       "0  a significant subset of natural language data ...   \n",
       "1  the ability to process such long sequences is ...   \n",
       "2  these tasks have important practical applicati...   \n",
       "3                              lo et al wang et al a   \n",
       "4  yet scaling state of the art models to long se...   \n",
       "\n",
       "                                  Sentences_Tokenize  \\\n",
       "0  [significant, subset, natural, language, data,...   \n",
       "1  [ability, process, long, sequences, critical, ...   \n",
       "2  [tasks, important, practical, applications, do...   \n",
       "3                         [lo, et, al, wang, et, al]   \n",
       "4  [yet, scaling, state, art, models, long, seque...   \n",
       "\n",
       "                                  Stemming_Sentences  \\\n",
       "0  [signific, subset, natur, languag, data, inclu...   \n",
       "1  [abil, process, long, sequenc, critic, mani, n...   \n",
       "2  [task, import, practic, applic, domain, scient...   \n",
       "3                         [lo, et, al, wang, et, al]   \n",
       "4  [yet, scale, state, art, model, long, sequenc,...   \n",
       "\n",
       "                                Lemmatized_Sentences  \n",
       "0  [significant, subset, natural, language, data,...  \n",
       "1  [ability, process, long, sequence, critical, m...  \n",
       "2  [task, important, practical, applications, dom...  \n",
       "3                         [lo, et, al, wang, et, al]  \n",
       "4  [yet, scale, state, art, model, long, sequence...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "def lemmatizing_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "df[\"Lemmatized_Sentences\"] = df[\"Sentences_Tokenize\"].apply(lemmatizing_tokens)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Best for the text clustering and document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>al</th>\n",
       "      <th>ammar</th>\n",
       "      <th>answer</th>\n",
       "      <th>applications</th>\n",
       "      <th>art</th>\n",
       "      <th>bert</th>\n",
       "      <th>challenge</th>\n",
       "      <th>classification</th>\n",
       "      <th>cohan</th>\n",
       "      <th>...</th>\n",
       "      <th>task</th>\n",
       "      <th>test</th>\n",
       "      <th>thousands</th>\n",
       "      <th>tokens</th>\n",
       "      <th>transformer</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>vaswani</th>\n",
       "      <th>wang</th>\n",
       "      <th>widely</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335549</td>\n",
       "      <td>0.278535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463739</td>\n",
       "      <td>0.217862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ability        al     ammar    answer  applications       art  bert  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000      0.000000  0.000000   0.0   \n",
       "1  0.197618  0.000000  0.000000  0.197618      0.000000  0.000000   0.0   \n",
       "2  0.000000  0.463739  0.217862  0.000000      0.217862  0.000000   0.0   \n",
       "3  0.000000  0.578008  0.000000  0.000000      0.000000  0.000000   0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000      0.000000  0.301032   0.0   \n",
       "\n",
       "   challenge  classification     cohan  ...      task      test  thousands  \\\n",
       "0   0.000000        0.000000  0.000000  ...  0.000000  0.000000   0.335549   \n",
       "1   0.000000        0.197618  0.000000  ...  0.140216  0.000000   0.000000   \n",
       "2   0.000000        0.000000  0.217862  ...  0.154580  0.000000   0.000000   \n",
       "3   0.000000        0.000000  0.000000  ...  0.000000  0.000000   0.000000   \n",
       "4   0.301032        0.000000  0.000000  ...  0.000000  0.301032   0.000000   \n",
       "\n",
       "     tokens  transformer  understand  use  vaswani      wang  widely  \n",
       "0  0.278535          0.0    0.000000  0.0      0.0  0.000000     0.0  \n",
       "1  0.000000          0.0    0.000000  0.0      0.0  0.000000     0.0  \n",
       "2  0.000000          0.0    0.217862  0.0      0.0  0.000000     0.0  \n",
       "3  0.000000          0.0    0.000000  0.0      0.0  0.407317     0.0  \n",
       "4  0.000000          0.0    0.000000  0.0      0.0  0.000000     0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  \n",
    "\n",
    "# Join the tokens back into a string before applying TF-IDF\n",
    "df[\"Lemmatized_Sentences_String\"] = df['Lemmatized_Sentences'].apply(' '.join)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Lemmatized_Sentences_String\"])  \n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display a sample of the transformed data\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF words saved to 'tfidf_words_lemmatized.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV file\n",
    "tfidf_df.to_csv(\"tfidf_words_lemmatized.csv\", index=False)\n",
    "\n",
    "print(\"TF-IDF words saved to 'tfidf_words_lemmatized.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings\n",
    "Best for finding similar words and performing NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=df[\"Lemmatized_Sentences\"], vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'art' not found in vocabulary.\n",
      "\n",
      "\n",
      "Most similar words to 'model':\n",
      "  sequence (0.17)\n",
      "  include (0.15)\n",
      "  many (0.14)\n",
      "  task (0.03)\n",
      "  et (0.00)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "example_text = [\"art\", \"model\"]\n",
    "\n",
    "for word in example_text:\n",
    "    if word in word2vec_model.wv:\n",
    "        similar_words = word2vec_model.wv.most_similar(word, topn=5) \n",
    "        print(f\"Most similar words to '{word}':\")\n",
    "        for sim_word, similarity in similar_words:\n",
    "            print(f\"  {sim_word} ({similarity:.2f})\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary.\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save into CSV file for Future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as csv file\n",
    "output_file = \"processed_data.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
